[
  {
    "objectID": "index.html#changing-the-axes",
    "href": "index.html#changing-the-axes",
    "title": "ggplot2",
    "section": "Changing the Axes",
    "text": "Changing the Axes\nCommon functions to alter the axes are (for the x-axis, replace with y for the y-axis):\n\nscale_x_continuous for continuous values\nscale_x_log10 for log-transforming the axis (not the values themselves)\n\nThey take, among others, the following options:\n\nlimits - range of values plotted\nbreaks - ticks displayed\nexpand - additional space before/after the plotted data\nguide - can add minor ticks, among others\n\n\nlibrary(ggprism)\ng1 &lt;- ggplot(cars, aes(x = speed, y = dist)) +\n  geom_point()\n\ng2 &lt;- ggplot(cars, aes(x = speed, y = dist)) +\n  geom_point() +\n  scale_x_continuous(limits = c(8, 16), breaks = seq(8, 16, 1)) + \n  scale_y_log10(guide = 'prism_minor')\n\nplot_grid(g1, g2)"
  },
  {
    "objectID": "index.html#changing-the-color-palette",
    "href": "index.html#changing-the-color-palette",
    "title": "ggplot2",
    "section": "Changing the Color Palette",
    "text": "Changing the Color Palette\nAs seen in Section 3, ggplot2 automatically colors factors specified by color or fill using a pre-defined palette. Two ways of changing the color palette of ggplot exist.\nOne way of changing the color palette is to change it globally via the ggplot2.discrete.colour option in options(). The colors are provided in a character vector by names or color hex codes.\n\noptions(ggplot2.discrete.colour = c('red', 'blue', 'green'))\n\nColors can also be changed on a per-plot basis using the scales _color_ and _fill_, for which a lot of pre-defined color palettes exist. Providing custom color palettes can be done with _color_manual and _fill_manual, for which the palette needs to be provided in a character vector.\n\nggblack &lt;- rgb(118, 113, 113, maxColorValue = 255) # gray\ngglime &lt;- rgb(140, 198, 63, maxColorValue = 255) # lime\nggred &lt;- rgb(241, 89, 42, maxColorValue = 255) # red\nggturquoise &lt;- rgb(37, 170, 225, maxColorValue = 255) # turqoise\nggpalette &lt;- c(ggblack, gglime, ggred, ggturquoise)\n\ng1 &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point()\n\ng2 &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point() +\n  scale_color_manual(values = ggpalette)\n\nplot_grid(g1, g2)"
  },
  {
    "objectID": "index.html#axes-labels-and-titles",
    "href": "index.html#axes-labels-and-titles",
    "title": "ggplot2",
    "section": "Axes, Labels, and Titles",
    "text": "Axes, Labels, and Titles\nAxes can be labeled using the functions xlab() and ylab(), and a title can be provided by the ggtitle() function. The same can be achieved with the labs() function, which additionally provides subtitles, captions, and legend titles, among other.\n\nggplot(iris, aes(x = Sepal.Width, y = Sepal.Length, colour = Species, pch = Petal.Length &gt; 3)) +\n  geom_point() +\n  labs(\n    title = 'Some General Description',\n    subtitle = 'Maybe some more detailed description',\n    caption = 'This label can be used to indicate the source',\n    pch = 'Change aesthetic \\nname in legend',\n    y = 'Length of Sepal'\n  )"
  },
  {
    "objectID": "index.html#themes",
    "href": "index.html#themes",
    "title": "ggplot2",
    "section": "Themes",
    "text": "Themes\ntheme_ allows to easily change the appearance of a plot using pre-defined settings. Available themes are:\n\n_gray()\n_bw()\n_linedraw()\n_light()\n_dark()\n_minimal()\n_classic()\n\nThese themes can be further modified, or new themes can be created, using the theme() function.\n\ng1 &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point()\n\ng2 &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point() +\n  theme_linedraw()\n\nplot_grid(g1, g2)"
  },
  {
    "objectID": "index.html#reusing-ggplot2-objects",
    "href": "index.html#reusing-ggplot2-objects",
    "title": "ggplot2",
    "section": "Reusing ggplot2 objects",
    "text": "Reusing ggplot2 objects\nIt may be useful to save several ggplot2 objects like geom_, scale_, and theme_ to use them for multiple plots. This can be achieved by saving the objects in a list and adding it to ggplot() using the + sign.\n\nchanges &lt;- list(\n  geom_point(),\n  theme_linedraw(),\n  ggtitle('title of scatter plot')\n)\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  changes"
  },
  {
    "objectID": "index.html#defining-functions-to-work-with-ggplot2-options",
    "href": "index.html#defining-functions-to-work-with-ggplot2-options",
    "title": "ggplot2",
    "section": "Defining functions to work with ggplot2 options",
    "text": "Defining functions to work with ggplot2 options\nSome options of ggplot2 functions allow a function as their value. The required input values are defined in the help file of the function.\n\nlibrary(stringr) # for str_wrap\n\ndata &lt;- data.frame(description = c(\"a very long text that requires linebreaks\", \n                                   'another text'), \n                   val = c(30, 300))\n\nformat_label &lt;- function(x) str_wrap(x, width = 20)\nformat_breaks &lt;- function(vec) seq(vec[1], vec[2], (vec[2]-vec[1])/5)\n\nggplot(data, aes(x = description, y = val)) +\n  geom_col() +\n  coord_flip() +\n  scale_x_discrete(labels = format_label) +\n  scale_y_continuous(breaks = format_breaks)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ggplot2",
    "section": "",
    "text": "A ggplot2 is initiated by the ggplot() command. When called on its own, it produces a graph without any data points. It is used to define options that will be forwarded to the functions responsible for adding data.\nA ggplot consists of layers that add graphical elements to the final plot. Layers are functions that begin with geom_ and define the type of plot. Examples are geom_point and geom_col, which create scatter and bar plots, respectively.\nThe general structure is as follows\n\nggplot() +\n  geom_"
  },
  {
    "objectID": "multivariate_statistics.html",
    "href": "multivariate_statistics.html",
    "title": "Multivariate Statistics",
    "section": "",
    "text": "This page is based on the lecture Applied Multivariate Statistics 401-0102-00L held by Fabio Sigrist at ETH Zurich."
  },
  {
    "objectID": "multivariate_statistics.html#factor-model",
    "href": "multivariate_statistics.html#factor-model",
    "title": "Multivariate Statistics",
    "section": "Factor Model",
    "text": "Factor Model\nThe factor model in matrix notation is as follows \\[\nX =\\pmb{\\Lambda} f + u\n\\] in which\n\n\\(X \\in \\mathbb{R}^q\\) are the observed variables,\n\\(\\pmb{\\Lambda} \\in \\mathbb{R}^{q\\times k}\\) matrix containing the factor loadings,\n\\(f \\in \\mathbb{R}^k\\) are the latent/common factors,\n\\(u \\in \\mathbb{R}^q\\) are the specific factors.\n\n\\(X\\), \\(u\\), and \\(f\\) are random vectors, while \\(\\Lambda\\) is constant. It is important to note that \\(f\\) is not observed, so \\(\\Lambda\\) and \\(u\\) cannot be estimated with a least squares approach (in contrast to a regression model).\nA number of things are assumed\n\n\\(E(u) = 0\\)\n\\(\\mathrm{Cov}(u) = \\Psi\\), a diagonal matrix. The specific factors are uncorrelated.\n\\(\\mathrm{Cov}(f_l, u_j) = 0\\), the common and specific factors are uncorrelated.\n\\(E(X) = 0\\), the data is mean-centered.\n\\(E(f) = 0\\), \\(\\mathrm{Cov}(f) = I\\), latent factors are standardized with a mean of zero, and a variance of one. They are uncorrelated.\n\nThese assumptions imply that the common factors and the observed variables are uncorrelated. The correlation is established by the latent factors \\(\\Lambda\\).\n\\[\\begin{align*}\n\\pmb{\\Sigma} &= \\mathrm{Cov}(\\pmb{\\Lambda} f+u) \\\\\n&= \\mathrm{Cov}(\\pmb{\\Lambda} f) + \\mathrm{Cov}(u) \\\\\n&= \\pmb{\\Lambda}\\mathrm{Cov}(f)\\pmb{\\Lambda}^T+\\pmb{\\Psi}\\\\\n&= \\pmb{\\Lambda\\Lambda}^T + \\pmb{\\Psi}\\\\\n\\end{align*}\\]\nWriting as scalars, this equals \\[\n\\sigma^2_j = \\Sigma_{l=1}^k \\lambda^2_{jl} + \\psi_j = h_j^2 + \\psi_j\n\\]\nin which \\(h_j^2\\) is the communality, the variance resulting from the latent factors, and \\(\\psi_j\\) the specific variance, that is not shared among the observed variables. If the above expression holds, then the factor model holds for \\(X\\).\nThe factor model is scale-invariant, meaning that scaling the loadings and specific variances after applying factor analysis is equal to applying factor analysis to scaled variables.\n\\[\\begin{align*}\n\\pmb{Y} &= \\pmb{CX}, \\pmb{C} = \\mathrm{diag}(c_j)\\\\\n\\mathrm{Cov}(\\pmb{Y}) &= \\pmb{C}\\mathrm{Cov}(\\pmb{X})\\pmb{C}^T \\\\\n&= (\\pmb{C\\Lambda})(\\pmb{C\\Lambda})^T + \\pmb{C\\Psi C}^T \\\\\n&= \\tilde{\\pmb{\\Lambda}}\\tilde{\\pmb{\\Lambda}}^T + \\tilde{\\pmb{\\Psi}}\\\\\n\\tilde{\\pmb{\\Lambda}} &= \\pmb{C\\Lambda}, \\tilde{\\pmb{\\Psi}} = \\pmb{C\\Psi C}^T\n\\end{align*}\\]"
  },
  {
    "objectID": "multivariate_statistics.html#model-definition",
    "href": "multivariate_statistics.html#model-definition",
    "title": "Multivariate Statistics",
    "section": "Model Definition",
    "text": "Model Definition\nThe idea is to transform the data matrix \\(\\pmb{X} \\in \\mathbb{R}^{n\\times q}\\) with the orthogonal loadings matrix \\(\\pmb{A} \\in \\mathbb{R}^{q\\times q}\\) such that the transformed values \\(\\pmb{Y} \\in \\mathbb{R}^{n\\times q}\\), called scores, have maximal variance along the axes. \\[\n\\pmb{Y} = \\pmb{X}\\pmb{A}\n\\] The k-th column in \\(\\pmb{Y}\\) and \\(\\pmb{A}\\) correspond to the scores and loadings of the k-th principle component, respectively. The loadings of each component \\(k\\) are normalized \\[\n\\Sigma_{j=1}^q a_{jk}^2 = 1\n\\] and the mean of each variable is zero. This can be achieved by subtracting the mean of the data. \\[\nE(X_1) = \\ldots = E(X_q) = 0\n\\] Principle components are ordered according to their variances, with the first principle component having the largest. In addition, all subsequent principle components have to be orthogonal to the previous components."
  },
  {
    "objectID": "multivariate_statistics.html#computation",
    "href": "multivariate_statistics.html#computation",
    "title": "Multivariate Statistics",
    "section": "Computation",
    "text": "Computation\nThere are multiple approaches to compute principle components, here, spectral decomposition of the covariance matrix \\(\\pmb{S} \\in \\mathbb{R}^{q\\times q}\\) will be used. The spectral decomposition \\[\n\\pmb{S} = \\pmb{A}\\pmb{D}\\pmb{A}^T\n\\] yields the matrix \\(\\pmb{A} \\in \\mathbb{R}^{q\\times q}\\), in which the columns \\(\\pmb{a}_k\\) are the eigenvectors, and the matrix \\(\\pmb{D} \\in \\mathbb{R}^{q\\times q}\\). \\(\\pmb{D}\\) is a diagonal matrix whose elements \\(\\lambda_k\\) are the eigenvalues to \\(\\pmb{a}_k\\) and correspond to the variance of the associated eigenvector. Thus \\[\n\\pmb{S}\\pmb{a}_k = \\lambda_k\\cdot\\pmb{a}_k\n\\] The algorithm to compute PCA with spectral decomposition is:\n\nCalculate the eigenvalues \\(\\lambda_k\\) and eigenvectors \\(\\pmb{a}_k\\) from the sample covariance matrix \\(\\pmb{S}\\)\nSort the eigenvalues in descending order and order the corresponding eigenvectors the same way\nThe k-th principle component is given by \\[\n\\pmb{y}_k = \\pmb{X}\\pmb{a}_k\n\\]"
  },
  {
    "objectID": "multivariate_statistics.html#properties",
    "href": "multivariate_statistics.html#properties",
    "title": "Multivariate Statistics",
    "section": "Properties",
    "text": "Properties\nThe sample variance of principle component \\(k\\) equals \\(\\lambda_k\\), the k-th largest eigenvalue of \\(\\pmb{S}\\). The total sample variance of all \\(q\\) variables equals \\[\n\\mathrm{Tr}\\left(\\pmb{S}\\right) = \\mathrm{Tr}\\left(\\pmb{D}\\right) = \\Sigma_{j=1}^q \\lambda_j\n\\] and the first \\(k\\) principle components explain a fraction of \\[\n\\frac{\\Sigma_{j=1}^k\\lambda_j}{\\Sigma_{j=1}^q\\lambda_j}\n\\] of the total variance. The sample correlations between different \\(\\pmb{y}_k\\) are zero, since the eigenvectors, by definition, are orthogonal. \\[\n\\mathrm{Cor}(\\pmb{y}_i, \\pmb{y}_j) = 0, i \\neq j\n\\] The signs of the loadings are arbitrary, since \\[\n\\pmb{S} = \\pmb{A}\\pmb{D}\\pmb{A}^T = (-\\pmb{A})\\pmb{D}(-\\pmb{A})^T\n\\]"
  },
  {
    "objectID": "multivariate_statistics.html#dimension-reduction",
    "href": "multivariate_statistics.html#dimension-reduction",
    "title": "Multivariate Statistics",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\nDimension reduction can be obtained by discarding principle components which account for a small fraction of the total variance. From \\(\\pmb{Y} = \\pmb{X}\\pmb{A}\\) it follows that \\(\\pmb{X} = \\pmb{Y}\\pmb{A}^T\\). \\(\\pmb{A}\\) and \\(\\pmb{Y}\\) can then be split into sub-matrices such that \\[\n\\pmb{X} = \\pmb{Y}\\pmb{A}^T = \\pmb{Y}^{(1)}\\pmb{A}_1^T + \\pmb{Y}^{(2)}\\pmb{A}_2^T \\approx \\pmb{Y}^{(1)}\\pmb{A}_1^T\n\\] if \\(\\pmb{Y}^{(2)}\\) is small and its variance is close to zero. The dimensions of the loadings sub-matrices are \\(\\dim(\\pmb{A_1}) = q \\times k\\) and \\(\\dim(\\pmb{A_2}) = q \\times (q-k)\\), with \\(k\\) the number of principle components kept. The dimensions of the score matrices are \\(\\dim(\\pmb{Y}^{(1)}) = n \\times k\\) and \\(\\dim(\\pmb{Y}^{(2)}) = n \\times (q-k)\\)"
  },
  {
    "objectID": "multivariate_statistics.html#practical-consideration",
    "href": "multivariate_statistics.html#practical-consideration",
    "title": "Multivariate Statistics",
    "section": "Practical Consideration",
    "text": "Practical Consideration\n\nInterpretation of Loadings\nWhen all variables are positively correlated, the first principle component is often a kind of average of the variables, while other principle components inform on the remaining patterns or shapes. Interpretation of principle components may be difficult, since all variables may have significant loadings within one principle component and can thus not be attributed to some particular set of variables.\n\n\nScaling\nPCA is not scale-invariant, as the scale (unit) in which the variable is measured changes the range of the values and with it its variance (e.g. centimeters vs. meters). If variables have large differences in their variances, the variable with the largest variances tends to dominate the first principle components. To overcome this issue, variables \\(X_j\\) may be scaled by dividing by their standard deviation \\(\\sigma_j\\), rendering the variances identical. \\[\nX_j' = \\frac{X_j}{\\sigma_j}\n\\] Scaling the data prior to PCA is often preferred, but involves the arbitrary choice to make variables equally important. Alternatively, PCA can be done using the correlation matrix \\(\\pmb{C}\\) instead of the covariance matrix, however, this is equal to scaling the data before calculating the covariance matrix.\n\n\nChoosing the Number of Principle Components\nThere are no universal rules to choose the number of principle components for dimension reduction, however, a number rules of thumb exist.\n\nThe cumulative proportion of explained variance by \\(k\\) principle components should be at least 0.8 \\[\n\\frac{\\Sigma_{j=1}^k\\lambda_j}{\\Sigma_{j=1}^q\\lambda_j} \\geq 0.8\n\\]\nOnly those principle components with above-average variance are kept. If the correlation matrix or scaled data was used to perform PCA, this corresponds to all principle components whose eigenvalues are larger than 1. \\[\n\\lambda_k \\geq \\frac{1}{q}\\Sigma_{j=1}^q\\lambda_j\n\\]\nLook at the scree plot of the variances \\(\\lambda_j\\) versus the component index \\(j\\) and keep only those principle components that occur before the elbow.\n\n\n\n\n\n\n\n\n\n\nThe effect of removing some principle components during dimension reduction may be different for different variables. To assess this effect, the squared correlations \\(r_{ij}^2\\) between variable \\(X_i\\) and principle component \\(Y_j\\) may be consulted, as \\(r_{ij}^2\\) corresponds to the proportion of variance of \\(X_i\\) that is explained by \\(Y_j\\)."
  },
  {
    "objectID": "multivariate_statistics.html#practical-considerations",
    "href": "multivariate_statistics.html#practical-considerations",
    "title": "Multivariate Statistics",
    "section": "Practical Considerations",
    "text": "Practical Considerations\n\nInterpretation of Loadings\nWhen all variables are positively correlated, the first principle component is often a kind of average of the variables, while other principle components inform on the remaining patterns or shapes. Interpretation of principle components may be difficult, since all variables may have significant loadings within one principle component and can thus not be attributed to some particular set of variables.\n\n\nScaling\nPCA is not scale-invariant, as the scale (unit) in which the variable is measured changes the range of the values and with it its variance (e.g. centimeters vs. meters). If variables have large differences in their variances, the variable with the largest variances tends to dominate the first principle components. To overcome this issue, variables \\(X_j\\) may be scaled by dividing by their standard deviation \\(\\sigma_j\\), rendering the variances identical. \\[\nX_j' = \\frac{X_j}{\\sigma_j}\n\\] Scaling the data prior to PCA is often preferred, but involves the arbitrary choice to make variables equally important. Alternatively, PCA can be done using the correlation matrix \\(\\pmb{C}\\) instead of the covariance matrix, however, this is equal to scaling the data before calculating the covariance matrix.\n\n\nChoosing the Number of Principle Components\nThere are no universal rules to choose the number of principle components for dimension reduction, however, a number rules of thumb exist.\n\nThe cumulative proportion of explained variance by \\(k\\) principle components should be at least 0.8 \\[\n\\frac{\\Sigma_{j=1}^k\\lambda_j}{\\Sigma_{j=1}^q\\lambda_j} \\geq 0.8\n\\]\nOnly those principle components with above-average variance are kept. If the correlation matrix or scaled data was used to perform PCA, this corresponds to all principle components whose eigenvalues are larger than 1. \\[\n\\lambda_k \\geq \\frac{1}{q}\\Sigma_{j=1}^q\\lambda_j\n\\]\nLook at the scree plot of the variances \\(\\lambda_j\\) versus the component index \\(j\\) and keep only those principle components that occur before the elbow.\n\n\n\n\n\n\n\n\n\n\nThe effect of removing some principle components during dimension reduction may be different for different variables. To assess this effect, the squared correlations \\(r_{ij}^2\\) between variable \\(X_i\\) and principle component \\(Y_j\\) may be consulted, as \\(r_{ij}^2\\) corresponds to the proportion of variance of \\(X_i\\) that is explained by \\(Y_j\\)."
  },
  {
    "objectID": "multivariate_statistics.html#predition-of-scores-for-new-observations",
    "href": "multivariate_statistics.html#predition-of-scores-for-new-observations",
    "title": "Multivariate Statistics",
    "section": "Predition of Scores for New Observations",
    "text": "Predition of Scores for New Observations\nThe score of a new observation \\(x = (x_1,\\ldots,x_q)^t\\) can be calculated with the previously obtained loadings matrix \\(\\pmb{A}\\). First, the new observation must be mean-subtracted with the sample-mean vector \\(\\hat{\\mu}\\). \\[\n\\tilde{x} = x-\\hat{\\mu} = (x_1 - \\hat{\\mu}_1, \\ldots, x_q - \\hat{\\mu}_q)^T\n\\] In case of scaling, the vector must be divided by the standard deviation vector \\(\\hat{\\sigma}\\). \\[\n\\tilde{x} = \\frac{x-\\hat{\\mu}}{\\hat{\\sigma}}\n\\] After pre-processing of the data, the scores of the new observation can be calculated by \\[\ny = \\pmb{A}^T\\tilde{x}\n\\]"
  },
  {
    "objectID": "multivariate_statistics.html#case-study",
    "href": "multivariate_statistics.html#case-study",
    "title": "Multivariate Statistics",
    "section": "Case Study",
    "text": "Case Study\nThe heptathlon dataset contains results from the olympic heptathlon competition in Seoul, 1988. It is a data frame with 25 observations on 8 variables: hurdles, highjump, shot, run200m, longjump, javelin, run800m, score. The variables hurdles, run200m, and run800m need to be transformed, such that a high value equals a good performance. This is done with the transformation max(variable)-variable.\n\n\n\n\n\n\n\nhurdles\nhighjump\nshot\nrun200m\nlongjump\njavelin\nrun800m\nscore\n\n\n\n\nJoyner-Kersee (USA)\n2.16\n1.86\n15.80\n4.05\n7.27\n45.66\n18.16\n7291\n\n\nJohn (GDR)\n2.00\n1.80\n16.23\n2.96\n6.71\n42.56\n20.55\n6897\n\n\nBehmer (GDR)\n1.65\n1.83\n14.20\n3.51\n6.68\n44.54\n22.47\n6858\n\n\nSablovskaite (URS)\n1.24\n1.80\n15.23\n2.69\n6.25\n42.78\n14.43\n6540\n\n\nChoubenkova (URS)\n1.34\n1.74\n14.76\n2.68\n6.32\n47.46\n18.77\n6540\n\n\nSchulz (GDR)\n1.10\n1.83\n13.50\n1.96\n6.33\n42.82\n20.88\n6411\n\n\nFleming (AUS)\n1.47\n1.80\n12.88\n3.02\n6.37\n40.28\n14.13\n6351\n\n\nGreiner (USA)\n1.30\n1.80\n14.13\n2.13\n6.47\n38.00\n13.02\n6297\n\n\nLajbnerova (CZE)\n1.22\n1.83\n14.28\n1.75\n6.11\n42.20\n10.62\n6252\n\n\nBouraga (URS)\n1.60\n1.77\n12.62\n3.02\n6.28\n39.06\n11.93\n6252\n\n\nWijnsma (HOL)\n1.10\n1.86\n13.01\n1.58\n6.34\n37.86\n15.18\n6205\n\n\nDimitrova (BUL)\n1.61\n1.80\n12.88\n3.02\n6.37\n40.28\n14.13\n6171\n\n\nScheider (SWI)\n1.00\n1.86\n11.58\n1.74\n6.05\n47.50\n11.74\n6137\n\n\nBraun (FRG)\n1.14\n1.83\n13.16\n1.83\n6.12\n44.58\n3.85\n6109\n\n\nRuotsalainen (FIN)\n1.06\n1.80\n12.32\n2.00\n6.08\n45.44\n9.61\n6101\n\n\nYuping (CHN)\n0.92\n1.86\n14.21\n1.61\n6.40\n38.60\n0.00\n6087\n\n\nHagger (GB)\n1.38\n1.80\n12.75\n1.14\n6.34\n35.76\n8.19\n5975\n\n\nBrown (USA)\n0.78\n1.83\n12.69\n1.78\n6.13\n44.34\n0.24\n5972\n\n\nMulliner (GB)\n0.46\n1.71\n12.68\n1.69\n6.10\n37.76\n8.65\n5746\n\n\nHautenauve (BEL)\n0.81\n1.77\n11.81\n1.00\n5.99\n35.68\n12.77\n5734\n\n\nKytola (FIN)\n0.54\n1.77\n11.66\n0.92\n5.75\n39.48\n13.32\n5686\n\n\nGeremias (BRA)\n0.62\n1.71\n12.95\n1.11\n5.50\n39.64\n2.65\n5508\n\n\nHui-Ing (TAI)\n0.00\n1.68\n10.00\n1.38\n5.47\n39.14\n9.37\n5290\n\n\nJeong-Mi (KOR)\n0.32\n1.71\n10.83\n0.00\n5.50\n39.26\n7.50\n5289\n\n\n\n\n\n\n\nThe correlation matrix indicates that some of the disciplines are well positively correlated, while others show very little correlation.\n\n\n\n\n\n\n\n\n\nThe variances of the variables\n\n\n hurdles highjump     shot  run200m longjump  javelin  run800m \n    0.26     0.00     2.24     0.88     0.16    12.03    37.79 \n\n\nare on very different scales, such that it is preferable to perform the analysis on the correlation matrix instead.\n\nsummary(heptathlon_pca &lt;- princomp(c_heptathlon[,-8], cor = TRUE), loadings = TRUE)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4     Comp.5\nStandard deviation     2.0793370 0.9481532 0.9109016 0.68319667 0.54618878\nProportion of Variance 0.6176632 0.1284278 0.1185345 0.06667967 0.04261745\nCumulative Proportion  0.6176632 0.7460909 0.8646255 0.93130515 0.97392260\n                           Comp.6      Comp.7\nStandard deviation     0.33745486 0.262042024\nProportion of Variance 0.01626797 0.009809432\nCumulative Proportion  0.99019057 1.000000000\n\nLoadings:\n         Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7\nhurdles   0.450         0.174         0.199  0.847       \nhighjump  0.315 -0.651  0.209 -0.557               -0.332\nshot      0.402         0.153  0.548 -0.672        -0.229\nrun200m   0.427  0.185 -0.130  0.231  0.618 -0.333 -0.470\nlongjump  0.451         0.270         0.122 -0.383  0.749\njavelin   0.242 -0.326 -0.881                       0.211\nrun800m   0.303  0.657 -0.193 -0.574 -0.319              \n\n\nThe “importance of the components” section informs on the standard deviation, the proportion of variance, and the cumulative proportion of the variance of each principle component. The cumulative proportion of the variance can be used to choose the number of dimensions for dimension reduction by choosing the first component \\(k\\) as cutoff for which this value is larger than 0.8.\nThe loadings indiciate the weight that is given to each variable for a given principle component. Principle component 1 has exclusively positive loadings and can be taken as an average of all values, while the remaining components indicate differences to that average. Thus, an athlete with a high score in principle component 2 does better than the average in run200m and run800m, but worse in highjump and javelin.\nThis is exemplified by the case of athlete Scheider (SWI), who has a score of principle component 1 close to zero and is therefore close to the average, but has a low score for principle component 2. Due to the double-negative of negative score and negative loadings, she does very well in highjump and javelin.\n\n\n\n\n\n\n\n\n\n\n\n\n\nhurdles\nhighjump\nshot\nrun200m\nlongjump\njavelin\nrun800m\nscore\n\n\n\n\naverage\n1.12\n1.79\n13.17\n2.02\n6.21\n41.28\n11.76\n6154.12\n\n\nScheider (SWI)\n1.00\n1.86\n11.58\n1.74\n6.05\n47.50\n11.74\n6137.00\n\n\n\n\n\n\n\nThe scores for an additional athlete can be predicted using the predict() function as follows\n\nathlete_new_data &lt;- data.frame(\n  hurdles = 3,\n  highjump = 1.5,\n  shot = 9.5,\n  run200m = 1.8,\n  longjump = 6.8,\n  javelin = 37,\n  run800m = 25\n)\npredict(heptathlon_pca, athlete_new_data)[,1:2]\n\n    Comp.1     Comp.2 \n-0.1903395  5.7786855"
  },
  {
    "objectID": "multivariate_statistics.html#pca-by-hand",
    "href": "multivariate_statistics.html#pca-by-hand",
    "title": "Multivariate Statistics",
    "section": "PCA by Hand",
    "text": "PCA by Hand\nTo illustrate how PCA works, PCA is done on the iris data set without using dedicated PCA functions. The iris data set contains four continuous and one categorical variable, only the continuous variables are considered. First, the variables need to be mean-subtracted\n\niris_centered &lt;- apply(iris[,1:4], 2, function(x) x-mean(x))\n\nAfterwards, the spectral decomposition of the covariance matrix is calculated using the eigen() function.\n\niris_eigen &lt;- iris_centered |&gt;\n  cov() |&gt;\n    eigen()\n\nThe eigenvalues correspond to the variance of the corresponding eigenvector. The eigenvector matrix is identical to the loadings calculated by printcomp(), except for the signs.\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\neigenvalues\n4.23\n0.24\n0.08\n0.02\n\n\n\n\n\n\n\n\nThe eigenvectors correspond to the loadings of the principal components\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\nSepal.Length\n0.361\n-0.657\n0.582\n0.315\n\n\nSepal.Width\n-0.085\n-0.730\n-0.598\n-0.320\n\n\nPetal.Length\n0.857\n0.173\n-0.076\n-0.480\n\n\nPetal.Width\n0.358\n0.075\n-0.546\n0.754\n\n\n\n\n\n\n\nThe scores are calculated from the data matrix and the eigenvector matrix\n\niris_centered%*%iris_eigen$vectors\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\n-2.68\n-0.32\n0.03\n0.00\n\n\n-2.71\n0.18\n0.21\n0.10\n\n\n-2.89\n0.14\n-0.02\n0.02\n\n\n-2.75\n0.32\n-0.03\n-0.08\n\n\n-2.73\n-0.33\n-0.09\n-0.06\n\n\n-2.28\n-0.74\n-0.17\n-0.02\n\n\n-2.82\n0.09\n-0.26\n-0.05\n\n\n-2.63\n-0.16\n0.02\n-0.05\n\n\n-2.89\n0.58\n-0.02\n-0.03\n\n\n-2.67\n0.11\n0.20\n-0.06\n\n\n-2.51\n-0.65\n0.08\n-0.02\n\n\n-2.61\n-0.01\n-0.10\n-0.16\n\n\n-2.79\n0.24\n0.21\n-0.01\n\n\n-3.22\n0.51\n-0.06\n-0.02\n\n\n-2.64\n-1.18\n0.15\n0.16\n\n\n-2.39\n-1.34\n-0.28\n0.01\n\n\n-2.62\n-0.81\n-0.14\n0.17\n\n\n-2.65\n-0.31\n-0.03\n0.08\n\n\n-2.20\n-0.87\n0.12\n0.03\n\n\n-2.59\n-0.51\n-0.21\n-0.07\n\n\n-2.31\n-0.39\n0.24\n-0.02\n\n\n-2.54\n-0.43\n-0.21\n0.04\n\n\n-3.22\n-0.13\n-0.29\n0.00\n\n\n-2.30\n-0.10\n-0.04\n0.15\n\n\n-2.36\n0.04\n-0.13\n-0.30\n\n\n-2.51\n0.15\n0.25\n0.03\n\n\n-2.47\n-0.13\n-0.09\n0.06\n\n\n-2.56\n-0.37\n0.08\n-0.01\n\n\n-2.64\n-0.31\n0.15\n0.07\n\n\n-2.63\n0.20\n-0.04\n-0.12\n\n\n-2.59\n0.20\n0.08\n-0.06\n\n\n-2.41\n-0.41\n0.15\n0.23\n\n\n-2.65\n-0.81\n-0.23\n-0.28\n\n\n-2.60\n-1.09\n-0.16\n-0.10\n\n\n-2.64\n0.12\n0.14\n0.02\n\n\n-2.87\n-0.07\n0.16\n0.16\n\n\n-2.63\n-0.60\n0.27\n0.18\n\n\n-2.80\n-0.27\n-0.09\n-0.17\n\n\n-2.98\n0.49\n-0.07\n-0.01\n\n\n-2.59\n-0.23\n0.08\n-0.01\n\n\n-2.77\n-0.26\n-0.08\n0.09\n\n\n-2.85\n0.94\n0.35\n0.32\n\n\n-3.00\n0.34\n-0.19\n-0.07\n\n\n-2.41\n-0.19\n-0.26\n0.18\n\n\n-2.21\n-0.44\n-0.30\n-0.18\n\n\n-2.71\n0.25\n0.10\n0.14\n\n\n-2.54\n-0.50\n-0.17\n-0.19\n\n\n-2.84\n0.23\n-0.08\n-0.06\n\n\n-2.54\n-0.58\n0.02\n-0.05\n\n\n-2.70\n-0.11\n0.09\n0.03\n\n\n1.28\n-0.69\n0.41\n0.02\n\n\n0.93\n-0.32\n0.02\n0.00\n\n\n1.46\n-0.50\n0.34\n0.00\n\n\n0.18\n0.83\n0.18\n0.09\n\n\n1.09\n-0.07\n0.31\n0.11\n\n\n0.64\n0.42\n-0.04\n-0.24\n\n\n1.10\n-0.28\n-0.17\n-0.08\n\n\n-0.75\n1.00\n-0.01\n-0.02\n\n\n1.04\n-0.23\n0.42\n-0.04\n\n\n-0.01\n0.72\n-0.28\n-0.01\n\n\n-0.51\n1.27\n0.27\n0.05\n\n\n0.51\n0.10\n-0.13\n0.05\n\n\n0.26\n0.55\n0.69\n0.06\n\n\n0.98\n0.12\n0.06\n-0.17\n\n\n-0.17\n0.25\n-0.09\n0.13\n\n\n0.93\n-0.47\n0.31\n0.10\n\n\n0.66\n0.35\n-0.33\n-0.19\n\n\n0.24\n0.33\n0.27\n-0.21\n\n\n0.94\n0.54\n0.50\n0.26\n\n\n0.05\n0.58\n0.24\n-0.04\n\n\n1.12\n0.08\n-0.46\n-0.08\n\n\n0.36\n0.07\n0.23\n0.12\n\n\n1.30\n0.33\n0.35\n0.00\n\n\n0.92\n0.18\n0.23\n-0.29\n\n\n0.71\n-0.15\n0.32\n0.04\n\n\n0.90\n-0.33\n0.32\n0.10\n\n\n1.33\n-0.24\n0.52\n0.04\n\n\n1.56\n-0.27\n0.16\n0.07\n\n\n0.81\n0.16\n-0.04\n-0.03\n\n\n-0.31\n0.37\n0.32\n0.07\n\n\n-0.07\n0.71\n0.24\n0.01\n\n\n-0.19\n0.68\n0.31\n-0.02\n\n\n0.14\n0.31\n0.18\n0.03\n\n\n1.38\n0.42\n-0.02\n-0.18\n\n\n0.59\n0.48\n-0.44\n-0.25\n\n\n0.81\n-0.19\n-0.39\n-0.11\n\n\n1.22\n-0.41\n0.24\n0.03\n\n\n0.82\n0.37\n0.61\n0.15\n\n\n0.25\n0.27\n-0.19\n-0.15\n\n\n0.17\n0.68\n0.06\n0.03\n\n\n0.46\n0.67\n0.02\n-0.27\n\n\n0.89\n0.03\n0.01\n-0.15\n\n\n0.23\n0.40\n0.23\n0.02\n\n\n-0.70\n1.01\n0.11\n0.05\n\n\n0.36\n0.50\n-0.02\n-0.10\n\n\n0.33\n0.21\n-0.08\n-0.24\n\n\n0.38\n0.29\n-0.08\n-0.13\n\n\n0.64\n-0.02\n0.21\n-0.02\n\n\n-0.91\n0.76\n0.01\n0.23\n\n\n0.30\n0.35\n-0.01\n-0.05\n\n\n2.53\n0.01\n-0.76\n-0.03\n\n\n1.42\n0.57\n-0.30\n-0.02\n\n\n2.62\n-0.34\n0.11\n0.07\n\n\n1.97\n0.18\n-0.11\n-0.24\n\n\n2.35\n0.04\n-0.29\n0.00\n\n\n3.40\n-0.55\n0.35\n-0.11\n\n\n0.52\n1.19\n-0.55\n-0.10\n\n\n2.93\n-0.36\n0.42\n-0.26\n\n\n2.32\n0.24\n0.35\n-0.08\n\n\n2.92\n-0.78\n-0.42\n0.11\n\n\n1.66\n-0.24\n-0.24\n0.12\n\n\n1.80\n0.22\n0.04\n0.08\n\n\n2.17\n-0.22\n-0.03\n0.16\n\n\n1.35\n0.78\n-0.28\n0.14\n\n\n1.59\n0.54\n-0.63\n0.33\n\n\n1.90\n-0.12\n-0.48\n0.22\n\n\n1.95\n-0.04\n-0.04\n-0.16\n\n\n3.49\n-1.18\n-0.13\n-0.31\n\n\n3.80\n-0.26\n0.51\n0.05\n\n\n1.30\n0.76\n0.34\n-0.05\n\n\n2.43\n-0.38\n-0.22\n0.19\n\n\n1.20\n0.61\n-0.51\n0.06\n\n\n3.50\n-0.46\n0.57\n-0.14\n\n\n1.39\n0.20\n0.06\n0.16\n\n\n2.28\n-0.33\n-0.29\n-0.06\n\n\n2.61\n-0.56\n0.21\n-0.24\n\n\n1.26\n0.18\n-0.05\n0.15\n\n\n1.29\n0.12\n-0.23\n0.00\n\n\n2.12\n0.21\n-0.15\n0.05\n\n\n2.39\n-0.46\n0.45\n-0.23\n\n\n2.84\n-0.38\n0.50\n-0.02\n\n\n3.23\n-1.37\n0.11\n-0.25\n\n\n2.16\n0.22\n-0.21\n0.13\n\n\n1.44\n0.14\n0.15\n-0.19\n\n\n1.78\n0.50\n0.17\n-0.51\n\n\n3.08\n-0.69\n0.34\n0.31\n\n\n2.14\n-0.14\n-0.73\n0.06\n\n\n1.91\n-0.05\n-0.16\n-0.22\n\n\n1.17\n0.16\n-0.28\n0.02\n\n\n2.11\n-0.37\n-0.03\n0.21\n\n\n2.31\n-0.18\n-0.32\n0.28\n\n\n1.92\n-0.41\n-0.11\n0.51\n\n\n1.42\n0.57\n-0.30\n-0.02\n\n\n2.56\n-0.28\n-0.29\n0.06\n\n\n2.42\n-0.30\n-0.50\n0.24\n\n\n1.94\n-0.19\n-0.18\n0.43\n\n\n1.53\n0.38\n0.12\n0.25\n\n\n1.76\n-0.08\n-0.13\n0.14\n\n\n1.90\n-0.12\n-0.72\n0.04\n\n\n1.39\n0.28\n-0.36\n-0.16\n\n\n\n\n\n\n\nDimension reduction is done by just taking the first two columns of the score matrix.\n\niris_scores &lt;- iris_centered%*%iris_eigen$vectors\ncolnames(iris_scores) &lt;- paste(\"Comp.\", 1:4, sep = \"\")\niris_scores[,1:2]"
  },
  {
    "objectID": "multivariate_statistics.html#estimation",
    "href": "multivariate_statistics.html#estimation",
    "title": "Multivariate Statistics",
    "section": "Estimation",
    "text": "Estimation\nBoth the factor loadings \\(\\pmb{\\Lambda}\\) and the specific variances \\(\\pmb{\\Psi}\\) require estimation. They are connected to the covariance matrix \\(\\pmb{\\Sigma}\\) by the expression \\[\n\\pmb{\\Sigma} = \\pmb{\\Lambda\\Lambda}^T + \\pmb{\\Psi}\n\\]\nEstimation of both \\(\\pmb{\\Lambda}\\) and \\(\\pmb{\\Psi}\\) from this equation starts by initializing \\(\\hat{\\pmb{\\Psi}}\\) with \\[\n\\hat{\\psi}_j = 1-\\hat{h}_j^2\n\\] in which \\(\\hat{h}_j^2\\) is either the square of the multiple correlation coefficient of the \\(j\\)-th variable with all other variables or the largest correlation coefficient between \\(j\\) and another variable. Subsequently, \\(\\pmb{\\Lambda}\\) can be estimated by spectral decomposition (using the correlation matrix \\(\\pmb{R}\\) instead of the covariance matrix) of \\[\n\\pmb{R}-\\hat{\\pmb{\\Psi}} = \\hat{\\pmb{\\Lambda}}\\hat{\\pmb{\\Lambda}}^T = \\pmb{A\\Omega A}^T = \\left(\\pmb{A\\Omega^\\frac{1}{2}}\\right) \\left(\\pmb{A\\Omega^\\frac{1}{2}}\\right)^T\n\\] Only the first \\(k\\) eigenvalues and eigenvectors are kept, corresponding to the number of factors chosen for the model. These reduced eigenvalues and eigenvectors are used to populate the loadings \\[\n\\hat{\\pmb{\\Lambda}}  = \\pmb{A_1\\Omega_1}^\\frac{1}{2}\n\\]\nThis estimate is then used to estimate the specific factors \\(\\hat{\\pmb{\\Psi}}\\) \\[\n\\hat{\\pmb{\\Psi}} = \\pmb{R} - \\hat{\\pmb{\\Lambda}}\\hat{\\pmb{\\Lambda}}^T\n\\] \\(\\hat{\\pmb{\\Lambda}}\\) and \\(\\hat{\\pmb{\\Psi}}\\) are iteratively re-estimated until convergence. Alternatively, parameter estimation can be done by using maximum likelyhood estimation, assuming that the variables follow a multivariate normal distribution.\n\\[\nl\\left(\\pmb{\\Psi}, \\pmb{\\Lambda}\\right) = -\\frac{n}{2}\\left(\\log\\left(2\\pi\\pmb{\\Sigma}\\right)+\\mathrm{Tr}\\left(\\pmb{\\Sigma}^{-1}S\\right)\\right)\n\\] Factor analysis using the maximum likelyhood approach is available in R via the factanal function."
  },
  {
    "objectID": "multivariate_statistics.html#heywood-cases",
    "href": "multivariate_statistics.html#heywood-cases",
    "title": "Multivariate Statistics",
    "section": "Heywood Cases",
    "text": "Heywood Cases\nIt is possible that some \\(\\hat{\\psi}_j &lt; 0\\) or \\(\\hat{\\psi}_j &gt; 1\\). These cases are called Heywood cases and violate that variances need to be positive and the normalization of the variance per variable \\[\n1 = \\hat{h}_j^2 + \\hat{\\psi}_j\n\\] Heywood cases can be approached by\n\ncollecting more data,\nusing a different model (number of factors, variables),\ntrying a different estimation technique."
  },
  {
    "objectID": "multivariate_statistics.html#number-of-factors",
    "href": "multivariate_statistics.html#number-of-factors",
    "title": "Multivariate Statistics",
    "section": "Number of Factors",
    "text": "Number of Factors\nHypothesis testing can be used to test whether the chosen number of factors is sufficient. The hypotheses are \\[\\begin{align*}\nH_0&: \\text{$k$-factor model holds true} \\\\\nH_1&: \\text{$\\pmb{\\Sigma}$ is more general (higher k necessary)}\\\\\n\\end{align*}\\] This hypothesis test included in the R output of factor analysis by default."
  },
  {
    "objectID": "multivariate_statistics.html#interpretation-of-factor-loadings",
    "href": "multivariate_statistics.html#interpretation-of-factor-loadings",
    "title": "Multivariate Statistics",
    "section": "Interpretation of Factor Loadings",
    "text": "Interpretation of Factor Loadings\nThere is no precise way of interpreting factor loadings, it can rather involve creativity and be debatable. Interpretation of factor loadings may be facilitated by rotations, which may change the loadings to be more interpretable. This is possible due to the non-uniqueness of a factor model.\nAssuming a rotation matrix \\(\\pmb{M}\\) and transformations \\(f^* = \\pmb{M}^Tf\\) and \\(\\pmb{\\Lambda}^* = \\pmb{\\Lambda}\\pmb{M}\\), it can be shown that the resulting factor model is identical to the untransformed model.\n\\[\\begin{align*}\n\\pmb{X}^* &= \\pmb{\\Lambda}^* f^* + u = \\left(\\pmb{\\Lambda}\\pmb{M}\\right)\\left(\\pmb{M}^T f\\right) + u = \\pmb{\\Lambda} f + u = \\pmb{X} \\\\\n\\pmb{\\Sigma}^* &= \\pmb{\\Lambda}^*{\\pmb{\\Lambda}^*}^T + \\pmb{\\Psi} = \\left(\\pmb{\\Lambda}\\pmb{M}\\right)\\left(\\pmb{\\Lambda}\\pmb{M}\\right)^T + \\pmb{\\Psi} = \\pmb{\\Lambda}\\pmb{\\Lambda}^T + \\pmb{\\Psi} =\\pmb{\\Sigma}\n\\end{align*}\\]\nNon-uniqueness can be approached by imposing restrictions on the model parametrization, which is commonly done by estimation procedures. In addition, the rotations mentioned above can be used to increase the interpretability of the factor loadings. Interpretation is made easier if every variable has a high loading on at most one factor, and if all factor loadings are either large and positive, or zero. Two types of rotations exist that attempt to acheive this.\n\nOrthogonal rotations require the factors to be uncorrelated. This is useful, since the loadings still represent covariances between latent factors and observed variables. The varimax rotation is a commonly used orthogonal rotation and default in the factanal function of R.\nOblique rotations allow the factors to be correlated, as it may lead to better interpretable factors. The promax rotation is an example for an oblique rotation which aims at a simple structure with low correlation between factors."
  },
  {
    "objectID": "multivariate_statistics.html#estimation-of-factor-scores",
    "href": "multivariate_statistics.html#estimation-of-factor-scores",
    "title": "Multivariate Statistics",
    "section": "Estimation of Factor Scores",
    "text": "Estimation of Factor Scores\nIt is possible to estimate factor scores for individual observations that represent scores of latent variables. Multiple methods of estimating factor scores exist, but they often yield very similar results in practice.\nThompson’s method estimates factor scores \\(\\hat{f}_i \\in \\mathbb{R}^k\\) by \\[\n\\hat{f}_i = \\hat{\\pmb{\\Lambda}}^T\\hat{\\pmb{\\Sigma}}^{-1}x_i\n\\] which is available via the option scores=\"regression\" in the R function factanal."
  },
  {
    "objectID": "multivariate_statistics.html#comparison-between-fa-pca",
    "href": "multivariate_statistics.html#comparison-between-fa-pca",
    "title": "Multivariate Statistics",
    "section": "Comparison between FA & PCA",
    "text": "Comparison between FA & PCA\nComparing the models of factor analysis and principle component analysis \\[\\begin{align*}\n\\text{FA: } \\pmb{X} &= \\pmb{\\Lambda}f + u\\\\\n\\text{PCA: } X &= \\pmb{A}_1^T Y^{(1)}+ \\pmb{A}_2^T Y^{(2)} =  \\pmb{A}_1^T Y^{(1)} + e\n\\end{align*}\\] shows that one of the differences is that \\(\\mathrm{Cov}(u)\\) is diagonal by assumption, while \\(\\mathrm{Cov}(e)\\) is usually not diagonal.\n\nFA and PCA reduce the dimensionality of data\nDo not yield results if the observed variables are uncorrelated\n\nPC components are similar to the original variables\nFA cannot find variance shared among the variables, thus the specific variance will be close to one\n\nThe results of PCA and FA become more similar, the smaller the specific variance is. The results become identical (up to rotation) if the specific variance is zero \\[\n\\pmb{\\Sigma} = \\pmb{\\Lambda}\\pmb{\\Lambda}^T + \\pmb{\\Psi} \\overset{\\pmb{\\Psi} = 0}{=}  \\pmb{\\Lambda}\\pmb{\\Lambda}^T\n\\] The resulting spectral decomposition is the same for both PCA and FA"
  },
  {
    "objectID": "multivariate_statistics.html#key-idea",
    "href": "multivariate_statistics.html#key-idea",
    "title": "Multivariate Statistics",
    "section": "Key Idea",
    "text": "Key Idea\nThe key idea of ICA to identify individual signals in a mixture is to use non-Gaussianity as measure of individual signals. This is motivated by the Central Limit Theorem, according to which mixtures of non-Gaussian distributions are more Gaussian than individual non-Gaussian distributions. By maximizing non-Gaussianity, individual sources can be separated from the mixtures.\nIndependent component analysis is a non-Gaussian alternative to PCA and FA. In addition, using the non-Gaussianity criterion allows to determine latent factors more uniquely than FA."
  },
  {
    "objectID": "multivariate_statistics.html#independent-component-model",
    "href": "multivariate_statistics.html#independent-component-model",
    "title": "Multivariate Statistics",
    "section": "Independent Component Model",
    "text": "Independent Component Model\nThe independent component model is a linear, and noise-free model, that assumes that a mixing matrix \\(\\pmb{B} \\in \\mathbb{R}^{q\\times k}\\) mixes the latent sources \\(\\pmb{S} \\in \\mathbb{R}^k\\) using linear combinations into the observed variables \\(\\pmb{X} \\in \\mathbb{R}^q\\), given that \\(E(\\pmb{X}) = 0\\).\n\\[\n\\pmb{X} = \\pmb{BS}\n\\]\nThe goal is to find an unmixing matrix \\(\\pmb{W} \\in \\mathbb{R}^{k\\times q}\\) that can separate the observed data into the latent sources. \\(\\pmb{X}\\) and \\(\\pmb{S}\\) are random vectors, while \\(\\pmb{B}\\) and \\(\\pmb{W}\\) are constant.\n\\[\n\\pmb{S} = \\pmb{WX}\n\\]\nObserved variables need to be pre-whitened to be used for ICA, as this reduces the number of parameters that have to be estimated. Pre-whitening transforms the observed data \\(\\pmb{X}\\) such that the transformed, pre-whitened variables \\(\\pmb{X}^W\\) have a variance of one and are uncorrelated to each other. Pre-whitening can be achieved by using the result of the spectral decomposition of the data covariance matrix \\(\\mathrm{Cov}(\\pmb{X}) = \\pmb{\\Sigma}\\). \\[\n\\pmb{X}^W = \\pmb{\\Sigma}^{-\\frac{1}{2}}\\pmb{X} = \\pmb{AD}^{-\\frac{1}{2}}\\pmb{A}^T\\pmb{X}\n\\]"
  },
  {
    "objectID": "multivariate_statistics.html#sec-fa",
    "href": "multivariate_statistics.html#sec-fa",
    "title": "Multivariate Statistics",
    "section": "Factor Model",
    "text": "Factor Model\nThe factor model in matrix notation is as follows \\[\nX =\\pmb{\\Lambda} f + u\n\\] in which\n\n\\(X \\in \\mathbb{R}^q\\) are the observed variables,\n\\(\\pmb{\\Lambda} \\in \\mathbb{R}^{q\\times k}\\) matrix containing the factor loadings,\n\\(f \\in \\mathbb{R}^k\\) are the latent/common factors,\n\\(u \\in \\mathbb{R}^q\\) are the specific factors.\n\n\\(X\\), \\(u\\), and \\(f\\) are random vectors, while \\(\\Lambda\\) is constant. It is important to note that \\(f\\) is not observed, so \\(\\Lambda\\) and \\(u\\) cannot be estimated with a least squares approach (in contrast to a regression model).\nA number of things are assumed\n\n\\(E(u) = 0\\)\n\\(\\mathrm{Cov}(u) = \\Psi\\), a diagonal matrix. The specific factors are uncorrelated.\n\\(\\mathrm{Cov}(f_l, u_j) = 0\\), the common and specific factors are uncorrelated.\n\\(E(X) = 0\\), the data is mean-centered.\n\\(E(f) = 0\\), \\(\\mathrm{Cov}(f) = I\\), latent factors are standardized with a mean of zero, and a variance of one. They are uncorrelated.\n\nThese assumptions imply that the common factors and the observed variables are uncorrelated. The correlation is established by the latent factors \\(\\Lambda\\).\n\\[\\begin{align*}\n\\pmb{\\Sigma} &= \\mathrm{Cov}(\\pmb{\\Lambda} f+u) \\\\\n&= \\mathrm{Cov}(\\pmb{\\Lambda} f) + \\mathrm{Cov}(u) \\\\\n&= \\pmb{\\Lambda}\\mathrm{Cov}(f)\\pmb{\\Lambda}^T+\\pmb{\\Psi}\\\\\n&= \\pmb{\\Lambda\\Lambda}^T + \\pmb{\\Psi}\\\\\n\\end{align*}\\]\nWriting as scalars, this equals \\[\n\\sigma^2_j = \\Sigma_{l=1}^k \\lambda^2_{jl} + \\psi_j = h_j^2 + \\psi_j\n\\]\nin which \\(h_j^2\\) is the communality, the variance resulting from the latent factors, and \\(\\psi_j\\) the specific variance, that is not shared among the observed variables. If the above expression holds, then the factor model holds for \\(X\\).\nThe factor model is scale-invariant, meaning that scaling the loadings and specific variances after applying factor analysis is equal to applying factor analysis to scaled variables.\n\\[\\begin{align*}\n\\pmb{Y} &= \\pmb{CX}, \\pmb{C} = \\mathrm{diag}(c_j)\\\\\n\\mathrm{Cov}(\\pmb{Y}) &= \\pmb{C}\\mathrm{Cov}(\\pmb{X})\\pmb{C}^T \\\\\n&= (\\pmb{C\\Lambda})(\\pmb{C\\Lambda})^T + \\pmb{C\\Psi C}^T \\\\\n&= \\tilde{\\pmb{\\Lambda}}\\tilde{\\pmb{\\Lambda}}^T + \\tilde{\\pmb{\\Psi}}\\\\\n\\tilde{\\pmb{\\Lambda}} &= \\pmb{C\\Lambda}, \\tilde{\\pmb{\\Psi}} = \\pmb{C\\Psi C}^T\n\\end{align*}\\]"
  }
]