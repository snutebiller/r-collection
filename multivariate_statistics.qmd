---
title: "Multivariate Statistics"
execute:
  echo: false
  warning: false
format:
  html:
    fig-align: center
    fig-width: 5
    fig-height: 3
editor_options: 
  chunk_output_type: console
---
```{r}
library(ggplot2)
library(kableExtra)
library(MVA)
library(corrplot)
gg_options <- list(
  theme_linedraw(base_size = 8),
  scale_color_manual(values = palette.colors(palette = 'Okabe-Ito')),
  scale_fill_manual(values = palette.colors(palette = 'Okabe-Ito'))
)
```

\newcommand\Cov{\mathrm{Cov}}
\newcommand\Cor{\mathrm{Cor}}
\newcommand\Tr[1]{\mathrm{Tr}\left(#1\right)}
\newcommand\mb[1]{\bm{#1}}
This page is based on the lecture Applied Multivariate Statistics [401-0102-00L](https://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2025S&ansicht=KATALOGDATEN&lerneinheitId=187848&lang=en) held by Fabio Sigrist at ETH Zurich.

# Principal Component Analysis
Principal component analysis (PCA) aims at reducing a large number of variables to a smaller number while preserving as much variation as possible. This can be useful for visualizing data, but also to reduce the computational burden for other statistical techniques as well as simplifying regression problems by reducing collinearity.

## Model Definition
The idea is to transform the data matrix $\pmb{X} \in \mathbb{R}^{n\times q}$ with the orthogonal loadings matrix $\pmb{A} \in \mathbb{R}^{q\times q}$ such that the transformed values $\pmb{Y} \in \mathbb{R}^{n\times q}$, called scores, have maximal variance along the axes.
$$
\pmb{Y} = \pmb{X}\pmb{A}
$$
The k-th column in $\pmb{Y}$ and $\pmb{A}$ correspond to the _scores_ and _loadings_ of the k-th principle component, respectively. The loadings of each component $k$ are normalized
$$
\Sigma_{j=1}^q a_{jk}^2 = 1
$$
and the mean of each variable is zero. This can be achieved by subtracting the mean of the data.
$$
E(X_1) = \ldots = E(X_q) = 0
$$
Principle components are ordered according to their variances, with the first principle component having the largest. In addition, all subsequent principle components have to be _orthogonal_ to the previous components.

## Computation
There are multiple approaches to compute principle components, here, spectral decomposition of the covariance matrix $\pmb{S} \in \mathbb{R}^{q\times q}$ will be used. The spectral decomposition
$$
\pmb{S} = \pmb{A}\pmb{D}\pmb{A}^T
$$
yields the matrix $\pmb{A} \in \mathbb{R}^{q\times q}$, in which the columns $\pmb{a}_k$ are the eigenvectors, and the matrix $\pmb{D} \in \mathbb{R}^{q\times q}$. $\pmb{D}$ is a diagonal matrix whose elements $\lambda_k$ are the eigenvalues to $\pmb{a}_k$ and correspond to the variance of the associated eigenvector. Thus
$$
\pmb{S}\pmb{a}_k = \lambda_k\cdot\pmb{a}_k
$$
The algorithm to compute PCA with spectral decomposition is:

1. Calculate the eigenvalues $\lambda_k$ and eigenvectors $\pmb{a}_k$ from the sample covariance matrix $\pmb{S}$
2. Sort the eigenvalues in descending order and order the corresponding eigenvectors the same way
3. The k-th principle component is given by
$$
\pmb{y}_k = \pmb{X}\pmb{a}_k
$$

## Properties
The sample variance of principle component $k$ equals $\lambda_k$, the k-th largest eigenvalue of $\pmb{S}$. The total sample variance of all $q$ variables equals
$$
\Tr{\pmb{S}} = \Tr{\pmb{D}} = \Sigma_{j=1}^q \lambda_j
$$
and the first $k$ principle components explain a fraction of
$$
\frac{\Sigma_{j=1}^k\lambda_j}{\Sigma_{j=1}^q\lambda_j}
$$
of the total variance. The sample correlations between different $\pmb{y}_k$ are zero, since the eigenvectors, by definition, are orthogonal.
$$
\Cor(\pmb{y}_i, \pmb{y}_j) = 0, i \neq j
$$
The signs of the loadings are arbitrary, since
$$
\pmb{S} = \pmb{A}\pmb{D}\pmb{A}^T = (-\pmb{A})\pmb{D}(-\pmb{A})^T
$$

## Dimension Reduction
Dimension reduction can be obtained by discarding principle components which account for a small fraction of the total variance. From $\pmb{Y} = \pmb{X}\pmb{A}$ it follows that $\pmb{X} = \pmb{Y}\pmb{A}^T$. $\pmb{A}$ and $\pmb{Y}$ can then be split into sub-matrices such that
$$
\pmb{X} = \pmb{Y}\pmb{A}^T = \pmb{Y}^{(1)}\pmb{A}_1^T + \pmb{Y}^{(2)}\pmb{A}_2^T \approx \pmb{Y}^{(1)}\pmb{A}_1^T
$$
if $\pmb{Y}^{(2)}$ is small and its variance is close to zero. The dimensions of the loadings sub-matrices are $\dim(\pmb{A_1}) = q \times k$ and $\dim(\pmb{A_2}) = q \times (q-k)$, with $k$ the number of principle components kept. The dimensions of the score matrices are $\dim(\pmb{Y}^{(1)}) = n \times k$ and $\dim(\pmb{Y}^{(2)}) = n \times (q-k)$

## Practical Considerations
### Interpretation of Loadings
When all variables are positively correlated, the first principle component is often a kind of average of the variables, while other principle components inform on the remaining patterns or shapes. Interpretation of principle components may be difficult, since all variables may have significant loadings within one principle component and can thus not be attributed to some particular set of variables.

### Scaling
PCA is _**not** scale-invariant_, as the scale (unit) in which the variable is measured changes the range of the values and with it its variance (e.g. centimeters vs. meters). If variables have large differences in their variances, the variable with the largest variances tends to dominate the first principle components. To overcome this issue, variables $X_j$ may be scaled by dividing by their standard deviation $\sigma_j$, rendering the variances identical.
$$
X_j' = \frac{X_j}{\sigma_j}
$$
Scaling the data prior to PCA is often preferred, but involves the arbitrary choice to make variables equally important. Alternatively, PCA can be done using the correlation matrix $\pmb{C}$ instead of the covariance matrix, however, this is equal to scaling the data before calculating the covariance matrix.

### Choosing the Number of Principle Components
There are no universal rules to choose the number of principle components for dimension reduction, however, a number rules of thumb exist.

1. The cumulative proportion of explained variance by $k$ principle components should be at least 0.8
$$
\frac{\Sigma_{j=1}^k\lambda_j}{\Sigma_{j=1}^q\lambda_j} \geq 0.8
$$
2. Only those principle components with above-average variance are kept. If the correlation matrix or scaled data was used to perform PCA, this corresponds to all principle components whose eigenvalues are larger than 1.
$$
\lambda_k \geq \frac{1}{q}\Sigma_{j=1}^q\lambda_j
$$
3. Look at the scree plot of the variances $\lambda_j$ versus the component index $j$ and keep only those principle components that occur before the elbow.
```{r}
ggplot(data.frame(x = 1:6, y = c(10*exp(-c(1:6)))), aes(x = x, y = y)) +
  geom_col() +
  geom_vline(xintercept = 2.5, linetype = 2) +
  geom_hline(yintercept = 1, linetype = 2) +
  geom_text(aes(x = x, y = y, label = label), 
            data.frame(x = c(2.5, 4),
                       y = c(3, 1),
                       label = c('Elbow:\nChoose PC until here', 'Above-average variance')),
            hjust = 'left',
            nudge_y = 0.2,
            nudge_x = 0.1) +
  scale_x_continuous(breaks = 1:6) +
  gg_options +
  labs(
    x = 'Principle Component j',
    y = 'Variance of PC j',
    title = 'Scree Plot'
  )
```
The effect of removing some principle components during dimension reduction may be different for different variables. To assess this effect, the squared correlations $r_{ij}^2$ between variable $X_i$ and principle component $Y_j$ may be consulted, as $r_{ij}^2$ corresponds to the proportion of variance of $X_i$ that is explained by $Y_j$.

## Predition of Scores for New Observations
The score of a new observation $x = (x_1,\ldots,x_q)^t$ can be calculated with the previously obtained loadings matrix $\pmb{A}$. First, the new observation must be mean-subtracted with the sample-mean vector $\hat{\mu}$.
$$
\tilde{x} = x-\hat{\mu} = (x_1 - \hat{\mu}_1, \ldots, x_q - \hat{\mu}_q)^T
$$
In case of scaling, the vector must be divided by the standard deviation vector $\hat{\sigma}$.
$$
\tilde{x} = \frac{x-\hat{\mu}}{\hat{\sigma}}
$$
After pre-processing of the data, the scores of the new observation can be calculated by
$$
y = \pmb{A}^T\tilde{x}
$$

## Case Study
The `heptathlon` dataset contains results from the olympic heptathlon competition in Seoul, 1988. It is a data frame with 25 observations on 8 variables: `{r} colnames(heptathlon)`. The variables hurdles, run200m, and run800m need to be transformed, such that a high value equals a good performance. This is done with the transformation `max(variable)-variable`. 

```{r}
c_heptathlon <- heptathlon[-25,]
c_heptathlon$hurdles <- with(c_heptathlon, max(hurdles)-hurdles)
c_heptathlon$run200m <- with(c_heptathlon, max(run200m)-run200m)
c_heptathlon$run800m <- with(c_heptathlon, max(run800m)-run800m)
kable(c_heptathlon) |>
  kable_styling() |>
    scroll_box(height = '200px')
```

The correlation matrix indicates that some of the disciplines are well positively correlated, while others show very little correlation.
```{r}
corrplot(cor(c_heptathlon[,-8]))
```
The variances of the variables
```{r}
round(diag(cov(c_heptathlon[,-8])), 2)
```
are on very different scales, such that it is preferable to perform the analysis on the correlation matrix instead.
```{r}
#| echo: TRUE

summary(heptathlon_pca <- princomp(c_heptathlon[,-8], cor = TRUE), loadings = TRUE)
```
```{r}
person_of_interest <- 'Scheider (SWI)'
```

The "importance of the components" section informs on the standard deviation, the proportion of variance, and the cumulative proportion of the variance of each principle component. The _cumulative proportion of the variance_ can be used to choose the number of dimensions for dimension reduction by choosing the first component $k$ as cutoff for which this value is larger than 0.8.

The loadings indiciate the weight that is given to each variable for a given principle component. Principle component 1 has exclusively positive loadings and can be taken as an average of all values, while the remaining components indicate differences to that average. Thus, an athlete with a high score in principle component 2 does better than the average in run200m and run800m, but worse in highjump and javelin.

This is exemplified by the case of athlete `{r} person_of_interest`, who has a score of principle component 1 close to zero and is therefore close to the average, but has a low score for principle component 2. Due to the double-negative of negative score and negative loadings, she does very well in highjump and javelin.

```{r}
hept_pca_df <- data.frame(
  heptathlon_pca$scores[,1:2],
  athlete = rownames(c_heptathlon),
  color = 'a'
)
hept_pca_df[hept_pca_df$athlete == person_of_interest,'color'] <- 'b'
colnames(hept_pca_df) <- c('PC1', 'PC2', 'athlete', 'color')
ggplot(hept_pca_df, aes(x = PC1, y = PC2, color = color)) +
  geom_point() +
  geom_text(aes(x = PC1, y = PC2, label = athlete), subset(hept_pca_df, athlete == person_of_interest), size = 3, nudge_y = -0.2) +
  gg_options +
  ggtitle(
    'The first two PCs of the heptathlon dataset'
  ) +
  theme(legend.position="none")

rbind(average = apply(c_heptathlon, 2, mean),
      c_heptathlon[which(rownames(c_heptathlon) == person_of_interest),]) |>
  round(2) |>
  kable() |>
    kable_styling()
```
The scores for an additional athlete can be predicted using the `predict()` function as follows
```{r}
#| echo: TRUE

athlete_new_data <- data.frame(
  hurdles = 3,
  highjump = 1.5,
  shot = 9.5,
  run200m = 1.8,
  longjump = 6.8,
  javelin = 37,
  run800m = 25
)
predict(heptathlon_pca, athlete_new_data)[,1:2]
```

## PCA by Hand
To illustrate how PCA works, PCA is done on the `iris` data set without using dedicated PCA functions. The `iris` data set contains four continuous and one categorical variable, only the continuous variables are considered. First, the variables need to be mean-subtracted
```{r}
#| echo: TRUE

iris_centered <- apply(iris[,1:4], 2, function(x) x-mean(x))
```

Afterwards, the spectral decomposition of the covariance matrix is calculated using the `eigen()` function.
```{r}
#| echo: TRUE

iris_eigen <- iris_centered |>
  cov() |>
    eigen()
```

The eigenvalues correspond to the variance of the corresponding eigenvector. The eigenvector matrix is identical to the loadings calculated by `printcomp()`, except for the signs.
```{r}
iris_eigen$values |>
  round(2) |>
  matrix(nrow = 1) |>
    as.data.frame(row.names = "eigenvalues") |>
      kable(col.names = paste('PC', 1:4, sep = "")) |>
        kable_styling()
iris_eigen$vectors |>
  round(3) |>
    as.data.frame(row.names = colnames(iris_centered)) |>
      kable(col.names = paste('PC', 1:4, sep = ""),
            caption = "The eigenvectors correspond to the loadings of the principal components") |>
        kable_styling()
```

The scores are calculated from the data matrix and the eigenvector matrix
```{r}
#| echo: TRUE
#| eval: FALSE

iris_centered%*%iris_eigen$vectors
```
```{r}
iris_centered%*%iris_eigen$vectors |>
  round(2) |>
  kable(col.names = paste("PC", 1:4, sep=""), row.names = FALSE) |>
    kable_styling() |>
      scroll_box(height = '200px')
```

Dimension reduction is done by just taking the first two columns of the score matrix.
```{r}
#| echo: TRUE
#| eval: FALSE

iris_scores <- iris_centered%*%iris_eigen$vectors
colnames(iris_scores) <- paste("Comp.", 1:4, sep = "")
iris_scores[,1:2]
```

```{r}
iris_scores <- iris_centered%*%iris_eigen$vectors |>
  as.data.frame() |>
  cbind(factor(iris$Species))
colnames(iris_scores) <- c(paste("Comp.", 1:4, sep = ""), "species")

ggplot(iris_scores, aes(x = Comp.1, y = Comp.2, col = species)) +
  geom_point() +
  labs(
    title = "Dimension reduction of the iris data set"
  ) +
  gg_options
```


# Factor Analysis
Factor analysis serves to find underlying latent factors to observed variables. This is useful for cases in which the variable of interest cannot be measured directly (e.g. intelligence, social class) but is represented by some proxy variable (e.g. test scores, occupation and education).

## Factor Model
The factor model in matrix notation is as follows
$$
X =\pmb{\Lambda} f + u
$$
in which

* $X \in \mathbb{R}^q$ are the observed variables,
* $\pmb{\Lambda} \in \mathbb{R}^{q\times k}$ matrix containing the factor loadings,
* $f \in \mathbb{R}^k$ are the latent/common factors,
* $u \in \mathbb{R}^q$ are the specific factors.

$X$, $u$, and $f$ are random vectors, while $\Lambda$ is constant. It is important to note that $f$ is not observed, so $\Lambda$ and $u$ cannot be estimated with a least squares approach (in contrast to a regression model).

A number of things are assumed

* $E(u) = 0$
* $\Cov (u) = \Psi$, a diagonal matrix. The specific factors are uncorrelated.
* $\Cov (f_l, u_j) = 0$, the common and specific factors are uncorrelated.
* $E(X) = 0$, the data is mean-centered.
* $E(f) = 0$, $\Cov (f) = I$, latent factors are standardized with a mean of zero, and a variance of one. They are uncorrelated.

These assumptions imply that the common factors and the observed variables are uncorrelated. The correlation is established by the latent factors $\Lambda$.

\begin{align*}
\pmb{\Sigma} &= \Cov(\pmb{\Lambda} f+u) \\
&= \Cov(\pmb{\Lambda} f) + \Cov(u) \\
&= \pmb{\Lambda}\Cov(f)\pmb{\Lambda}^T+\pmb{\Psi}\\
&= \pmb{\Lambda\Lambda}^T + \pmb{\Psi}\\
\end{align*}

Writing as scalars, this equals
$$
\sigma^2_j = \Sigma_{l=1}^k \lambda^2_{jl} + \psi_j = h_j^2 + \psi_j
$$

in which $h_j^2$ is the communality, the variance resulting from the latent factors, and $\psi_j$ the specific variance, that is not shared among the observed variables. If the above expression holds, then the factor model holds for $X$. 

The factor model is scale-invariant, meaning that scaling the loadings and specific variances after applying factor analysis is equal to applying factor analysis to scaled variables.

\begin{align*}
\pmb{Y} &= \pmb{CX}, \pmb{C} = \mathrm{diag}(c_j)\\
\Cov(\pmb{Y}) &= \pmb{C}\Cov(\pmb{X})\pmb{C}^T \\
&= (\pmb{C\Lambda})(\pmb{C\Lambda})^T + \pmb{C\Psi C}^T \\
&= \tilde{\pmb{\Lambda}}\tilde{\pmb{\Lambda}}^T + \tilde{\pmb{\Psi}}\\
\tilde{\pmb{\Lambda}} &= \pmb{C\Lambda}, \tilde{\pmb{\Psi}} = \pmb{C\Psi C}^T
\end{align*}

## Estimation
Both the factor loadings $\pmb{\Lambda}$ and the specific variances $\pmb{\Psi}$ require estimation. They are connected to the covariance matrix $\pmb{\Sigma}$ by the expression
$$
\pmb{\Sigma} = \pmb{\Lambda\Lambda}^T + \pmb{\Psi}
$$

Estimation of both $\pmb{\Lambda}$ and $\pmb{\Psi}$ from this equation starts by initializing $\hat{\pmb{\Psi}}$ with
$$
\hat{\psi}_j = 1-\hat{h}_j^2
$$
in which $\hat{h}_j^2$ is either the square of the multiple correlation coefficient of the $j$-th variable with all other variables or the largest correlation coefficient between $j$ and another variable. Subsequently, $\pmb{\Lambda}$ can be estimated by spectral decomposition (using the correlation matrix $\pmb{R}$ instead of the covariance matrix) of
$$
\pmb{R}-\hat{\pmb{\Psi}} = \hat{\pmb{\Lambda}}\hat{\pmb{\Lambda}}^T = \pmb{A\Omega A}^T = \left(\pmb{A\Omega^\frac{1}{2}}\right) \left(\pmb{A\Omega^\frac{1}{2}}\right)^T
$$
Only the first $k$ eigenvalues and eigenvectors are kept, corresponding to the number of factors chosen for the model. These reduced eigenvalues and eigenvectors are used to populate the loadings \pmb{\Lambda}
$$
\hat{\pmb{\Lambda}}  = \pmb{A_1\Omega_1}^\frac{1}{2}
$$

This estimate is then used to estimate the specific factors $\hat{\pmb{\Psi}}$
$$
\hat{\pmb{\Psi}} = \pmb{R} - \hat{\pmb{\Lambda}}\hat{\pmb{\Lambda}}^T 
$$

