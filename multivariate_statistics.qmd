---
title: "Multivariate Statistics"
execute:
  echo: true
  warning: false
format:
  html:
    fig-align: center
    fig-width: 5
    fig-height: 3
editor_options: 
  chunk_output_type: console
---
\newcommand\Cov{\mathrm{Cov}}
\newcommand\Cor{\mathrm{Cor}}
\newcommand\Tr[1]{\mathrm{Tr}\left(#1\right)}
\newcommand\mb[1]{\bm{#1}}
This page is based on the lecture Applied Multivariate Statistics [401-0102-00L](https://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2025S&ansicht=KATALOGDATEN&lerneinheitId=187848&lang=en) held by Fabio Sigrist at ETH Zurich.

# Principal Component Analysis
Principal component analysis (PCA) aims at reducing a large number of variables to a smaller number while preserving as much variation as possible. This can be useful for visualizing data, but also to reduce the computational burden for other statistical techniques as well as simplifying regression problems by reducing collinearity.

## Model Definition
The idea is to transform the data matrix $\pmb{X} \in \mathbb{R}^{n\times q}$ with the loadings matrix $\pmb{A} \in \mathbb{R}^{q\times q}$ such that the transformed values $\pmb{Y} \in \mathbb{R}^{n\times q}$, called scores, have maximal variance along the axes.
$$
\pmb{Y} = \pmb{X}\pmb{A}
$$
The k-th column in $\pmb{Y}$ and $\pmb{A}$ correspond to the _scores_ and _loadings_ of the k-th principle component, respectively. The loadings of each component $k$ are normalized
$$
\Sigma_{j=1}^q a_{jk}^2 = 1
$$
and the mean of each variable is zero. This can be achieved by subtracting the mean of the data.
$$
E(X_1) = \ldots = E(X_q) = 0
$$
Principle components are ordered according to their variances, with the first principle component having the largest. In addition, all subsequent principle components have to be _orthogonal_ to the previous components.

## Computation
There are multiple approaches to compute principle components, here, spectral decomposition of the covariance matrix $\pmb{S} \in \mathbb{R}^{q\times q}$ will be used. The spectral decomposition
$$
\pmb{S} = \pmb{A}\pmb{D}\pmb{A}^T
$$
yields the matrix $\pmb{A} \in \mathbb{R}^{q\times q}$, in which the columns $\pmb{a}_k$ are the eigenvectors, and the matrix $\pmb{D} \in \mathbb{R}^{q\times q}$. $\pmb{D}$ is a diagonal matrix whose elements $\lambda_k$ are the eigenvalues to $\pmb{a}_k$ and correspond to the variance of the associated eigenvector. Thus
$$
\pmb{S}\pmb{a}_k = \lambda_k\cdot\pmb{a}_k
$$
The algorithm to compute PCA with spectral decomposition is:

1. Calculate the eigenvalues $\lambda_k$ and eigenvectors $\pmb{a}_k$ from the sample covariance matrix $\pmb{S}$
2. Sort the eigenvalues in descending order and order the corresponding eigenvectors the same way
3. The k-th principle component is given by
$$
\pmb{y}_k = \pmb{X}\pmb{a}_k
$$

## Properties
The sample variance of principle component $k$ equals $\lambda_k$, the k-th largest eigenvalue of $\pmb{S}$. The total sample variance of all $q$ variables equals
$$
\Tr{\pmb{S}} = \Tr{\pmb{D}} = \Sigma_{j=1}^q \lambda_j
$$
and the first $k$ principle components explain a fraction of
$$
\frac{\Sigma_{j=1}^k\lambda_j}{\Sigma_{j=1}^q\lambda_j}
$$
of the total variance. The sample correlations between different $\pmb{y}_k$ are zero, since the eigenvectors, by definition, are orthogonal.
$$
\Cor(\pmb{y}_i, \pmb{y}_j) = 0, i \neq j
$$

# Factor Analysis
Factor analysis serves to find underlying latent factors to observed variables. This is useful for cases in which the variable of interest cannot be measured directly (e.g. intelligence, social class) but is represented by some proxy variable (e.g. test scores, occupation and education).

## Factor Model
The factor model in matrix notation is as follows
$$
X = \Lambda f + u
$$
in which

* $X \in \mathbb{R}^q$ are the observed variables,
* $\Lambda$ is a $q\times k$ matrix containing the factor loadings,
* $f \in \mathbb{R}^k$ are the latent/common factors,
* $u \in \mathbb{R}^q$ are the specific factors.

$X$, $u$, and $f$ are random vectors, while $\Lambda$ is constant. It is important to note that $f$ is not observed, so $\Lambda$ and $u$ cannot be estimated with a least squares approach (in contrast to a regression model).

A number of things are assumed

* $E(u) = 0$
* $\Cov (u) = \Psi$, a diagonal matrix. The specific factors are uncorrelated.
* $\Cov (f_l, u_j) = 0$, the common and specific factors are uncorrelated.
* $E(X) = 0$, the data is mean-centered.
* $E(f) = 0$, $\Cov (f) = I$, latent factors are standardized with a mean of zero, and a variance of one. They are uncorrelated.

These assumptions imply that the common factors and the observed variables are uncorrelated. The correlation is established by the latent factors $\Lambda$.

\begin{align*}
\Sigma &= \Cov(\Lambda f+u) \\
&= \Cov(\Lambda f) + \Cov(u) \\
&= \Lambda\Cov(f)\Lambda^T+\Psi\\
&= \Lambda\Lambda^T + \Psi\\
\end{align*}

Writing as scalars, this equals
$$
\sigma^2_j = \Sigma_{l=1}^k \lambda^2_{jl} + \psi_j = h_j^2 + \psi_j
$$

in which $h_j^2$ is the communality, the variance resulting from the latent factors, and $\psi_j$ the specific variance, that is not shared among the observed variables. If the above expression holds, then the factor model holds for $X$. 
