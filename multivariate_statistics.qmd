---
title: "Multivariate Statistics"
execute:
  echo: false
  warning: false
format:
  html:
    fig-align: center
    fig-width: 5
    fig-height: 3
editor_options: 
  chunk_output_type: console
---
```{r}
library(ggplot2)
library(kableExtra)
library(MVA)
library(corrplot)
library(cowplot)
gg_options <- list(
  theme_linedraw(base_size = 8),
  scale_color_manual(values = palette.colors(palette = 'Okabe-Ito')),
  scale_fill_manual(values = palette.colors(palette = 'Okabe-Ito'))
)
```

\newcommand\Cov{\mathrm{Cov}}
\newcommand\Cor{\mathrm{Cor}}
\newcommand\Tr[1]{\mathrm{Tr}\left(#1\right)}
\newcommand\mb[1]{\bm{#1}}
\DeclareMathOperator*{\argmax}{\arg\!\max}

This page is based on the lecture Applied Multivariate Statistics [401-0102-00L](https://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2025S&ansicht=KATALOGDATEN&lerneinheitId=187848&lang=en) held by Fabio Sigrist at ETH Zurich.

# Principal Component Analysis {#sec-pca}
Principal component analysis (PCA) aims at reducing a large number of variables to a smaller number while preserving as much variation as possible. This can be useful for visualizing data, but also to reduce the computational burden for other statistical techniques as well as simplifying regression problems by reducing collinearity.

## Model Definition
The idea is to transform the data matrix $\pmb{X} \in \mathbb{R}^{n\times q}$ with the orthogonal loadings matrix $\pmb{A} \in \mathbb{R}^{q\times q}$ such that the transformed values $\pmb{Y} \in \mathbb{R}^{n\times q}$, called scores, have maximal variance along the axes.
$$
\pmb{Y} = \pmb{X}\pmb{A}
$$
The k-th column in $\pmb{Y}$ and $\pmb{A}$ correspond to the _scores_ and _loadings_ of the k-th principle component, respectively. The loadings of each component $k$ are normalized
$$
\Sigma_{j=1}^q a_{jk}^2 = 1
$$
and the mean of each variable is zero. This can be achieved by subtracting the mean of the data.
$$
E(X_1) = \ldots = E(X_q) = 0
$$
Principle components are ordered according to their variances, with the first principle component having the largest. In addition, all subsequent principle components have to be _orthogonal_ to the previous components.

## Computation
There are multiple approaches to compute principle components, here, spectral decomposition of the covariance matrix $\pmb{S} \in \mathbb{R}^{q\times q}$ will be used. The spectral decomposition
$$
\pmb{S} = \pmb{A}\pmb{D}\pmb{A}^T
$$
yields the matrix $\pmb{A} \in \mathbb{R}^{q\times q}$, in which the columns $\pmb{a}_k$ are the eigenvectors, and the matrix $\pmb{D} \in \mathbb{R}^{q\times q}$. $\pmb{D}$ is a diagonal matrix whose elements $\lambda_k$ are the eigenvalues to $\pmb{a}_k$ and correspond to the variance of the associated eigenvector. Thus
$$
\pmb{S}\pmb{a}_k = \lambda_k\cdot\pmb{a}_k
$$
The algorithm to compute PCA with spectral decomposition is:

1. Calculate the eigenvalues $\lambda_k$ and eigenvectors $\pmb{a}_k$ from the sample covariance matrix $\pmb{S}$
2. Sort the eigenvalues in descending order and order the corresponding eigenvectors the same way
3. The k-th principle component is given by
$$
\pmb{y}_k = \pmb{X}\pmb{a}_k
$$

## Properties
The sample variance of principle component $k$ equals $\lambda_k$, the k-th largest eigenvalue of $\pmb{S}$. The total sample variance of all $q$ variables equals
$$
\Tr{\pmb{S}} = \Tr{\pmb{D}} = \Sigma_{j=1}^q \lambda_j
$$
and the first $k$ principle components explain a fraction of
$$
\frac{\Sigma_{j=1}^k\lambda_j}{\Sigma_{j=1}^q\lambda_j}
$$
of the total variance. The sample correlations between different $\pmb{y}_k$ are zero, since the eigenvectors, by definition, are orthogonal.
$$
\Cor(\pmb{y}_i, \pmb{y}_j) = 0, i \neq j
$$
The signs of the loadings are arbitrary, since
$$
\pmb{S} = \pmb{A}\pmb{D}\pmb{A}^T = (-\pmb{A})\pmb{D}(-\pmb{A})^T
$$

## Dimension Reduction
Dimension reduction can be obtained by discarding principle components which account for a small fraction of the total variance. From $\pmb{Y} = \pmb{X}\pmb{A}$ it follows that $\pmb{X} = \pmb{Y}\pmb{A}^T$. $\pmb{A}$ and $\pmb{Y}$ can then be split into sub-matrices such that
$$
\pmb{X} = \pmb{Y}\pmb{A}^T = \pmb{Y}^{(1)}\pmb{A}_1^T + \pmb{Y}^{(2)}\pmb{A}_2^T \approx \pmb{Y}^{(1)}\pmb{A}_1^T
$$
if $\pmb{Y}^{(2)}$ is small and its variance is close to zero. The dimensions of the loadings sub-matrices are $\dim(\pmb{A_1}) = q \times k$ and $\dim(\pmb{A_2}) = q \times (q-k)$, with $k$ the number of principle components kept. The dimensions of the score matrices are $\dim(\pmb{Y}^{(1)}) = n \times k$ and $\dim(\pmb{Y}^{(2)}) = n \times (q-k)$

## Practical Considerations
### Interpretation of Loadings
When all variables are positively correlated, the first principle component is often a kind of average of the variables, while other principle components inform on the remaining patterns or shapes. Interpretation of principle components may be difficult, since all variables may have significant loadings within one principle component and can thus not be attributed to some particular set of variables.

### Scaling
PCA is _**not** scale-invariant_, as the scale (unit) in which the variable is measured changes the range of the values and with it its variance (e.g. centimeters vs. meters). If variables have large differences in their variances, the variable with the largest variances tends to dominate the first principle components. To overcome this issue, variables $X_j$ may be scaled by dividing by their standard deviation $\sigma_j$, rendering the variances identical.
$$
X_j' = \frac{X_j}{\sigma_j}
$$
Scaling the data prior to PCA is often preferred, but involves the arbitrary choice to make variables equally important. Alternatively, PCA can be done using the correlation matrix $\pmb{C}$ instead of the covariance matrix, however, this is equal to scaling the data before calculating the covariance matrix.

### Choosing the Number of Principle Components
There are no universal rules to choose the number of principle components for dimension reduction, however, a number rules of thumb exist.

1. The cumulative proportion of explained variance by $k$ principle components should be at least 0.8
$$
\frac{\Sigma_{j=1}^k\lambda_j}{\Sigma_{j=1}^q\lambda_j} \geq 0.8
$$
2. Only those principle components with above-average variance are kept. If the correlation matrix or scaled data was used to perform PCA, this corresponds to all principle components whose eigenvalues are larger than 1.
$$
\lambda_k \geq \frac{1}{q}\Sigma_{j=1}^q\lambda_j
$$
3. Look at the scree plot of the variances $\lambda_j$ versus the component index $j$ and keep only those principle components that occur before the elbow.
```{r}
ggplot(data.frame(x = 1:6, y = c(10*exp(-c(1:6)))), aes(x = x, y = y)) +
  geom_col() +
  geom_vline(xintercept = 2.5, linetype = 2) +
  geom_hline(yintercept = 1, linetype = 2) +
  geom_text(aes(x = x, y = y, label = label), 
            data.frame(x = c(2.5, 4),
                       y = c(3, 1),
                       label = c('Elbow:\nChoose PC until here', 'Above-average variance')),
            hjust = 'left',
            nudge_y = 0.2,
            nudge_x = 0.1) +
  scale_x_continuous(breaks = 1:6) +
  gg_options +
  labs(
    x = 'Principle Component j',
    y = 'Variance of PC j',
    title = 'Scree Plot'
  )
```
The effect of removing some principle components during dimension reduction may be different for different variables. To assess this effect, the squared correlations $r_{ij}^2$ between variable $X_i$ and principle component $Y_j$ may be consulted, as $r_{ij}^2$ corresponds to the proportion of variance of $X_i$ that is explained by $Y_j$.

## Predition of Scores for New Observations
The score of a new observation $x = (x_1,\ldots,x_q)^t$ can be calculated with the previously obtained loadings matrix $\pmb{A}$. First, the new observation must be mean-subtracted with the sample-mean vector $\hat{\mu}$.
$$
\tilde{x} = x-\hat{\mu} = (x_1 - \hat{\mu}_1, \ldots, x_q - \hat{\mu}_q)^T
$$
In case of scaling, the vector must be divided by the standard deviation vector $\hat{\sigma}$.
$$
\tilde{x} = \frac{x-\hat{\mu}}{\hat{\sigma}}
$$
After pre-processing of the data, the scores of the new observation can be calculated by
$$
y = \pmb{A}^T\tilde{x}
$$

## Case Study
The `heptathlon` dataset contains results from the olympic heptathlon competition in Seoul, 1988. It is a data frame with 25 observations on 8 variables: `{r} colnames(heptathlon)`. The variables hurdles, run200m, and run800m need to be transformed, such that a high value equals a good performance. This is done with the transformation `max(variable)-variable`. 

```{r}
c_heptathlon <- heptathlon[-25,]
c_heptathlon$hurdles <- with(c_heptathlon, max(hurdles)-hurdles)
c_heptathlon$run200m <- with(c_heptathlon, max(run200m)-run200m)
c_heptathlon$run800m <- with(c_heptathlon, max(run800m)-run800m)
kable(c_heptathlon) |>
  kable_styling() |>
    scroll_box(height = '200px')
```

The correlation matrix indicates that some of the disciplines are well positively correlated, while others show very little correlation.
```{r}
corrplot(cor(c_heptathlon[,-8]))
```
The variances of the variables
```{r}
round(diag(cov(c_heptathlon[,-8])), 2)
```
are on very different scales, such that it is preferable to perform the analysis on the correlation matrix instead.
```{r}
#| echo: TRUE

summary(heptathlon_pca <- princomp(c_heptathlon[,-8], cor = TRUE), loadings = TRUE)
```
```{r}
person_of_interest <- 'Scheider (SWI)'
```

The "importance of the components" section informs on the standard deviation, the proportion of variance, and the cumulative proportion of the variance of each principle component. The _cumulative proportion of the variance_ can be used to choose the number of dimensions for dimension reduction by choosing the first component $k$ as cutoff for which this value is larger than 0.8.

The loadings indiciate the weight that is given to each variable for a given principle component. Principle component 1 has exclusively positive loadings and can be taken as an average of all values, while the remaining components indicate differences to that average. Thus, an athlete with a high score in principle component 2 does better than the average in run200m and run800m, but worse in highjump and javelin.

This is exemplified by the case of athlete `{r} person_of_interest`, who has a score of principle component 1 close to zero and is therefore close to the average, but has a low score for principle component 2. Due to the double-negative of negative score and negative loadings, she does very well in highjump and javelin.

```{r}
hept_pca_df <- data.frame(
  heptathlon_pca$scores[,1:2],
  athlete = rownames(c_heptathlon),
  color = 'a'
)
hept_pca_df[hept_pca_df$athlete == person_of_interest,'color'] <- 'b'
colnames(hept_pca_df) <- c('PC1', 'PC2', 'athlete', 'color')
ggplot(hept_pca_df, aes(x = PC1, y = PC2, color = color)) +
  geom_point() +
  geom_text(aes(x = PC1, y = PC2, label = athlete), subset(hept_pca_df, athlete == person_of_interest), size = 3, nudge_y = -0.2) +
  gg_options +
  ggtitle(
    'The first two PCs of the heptathlon dataset'
  ) +
  theme(legend.position="none")

rbind(average = apply(c_heptathlon, 2, mean),
      c_heptathlon[which(rownames(c_heptathlon) == person_of_interest),]) |>
  round(2) |>
  kable() |>
    kable_styling()
```
The scores for an additional athlete can be predicted using the `predict()` function as follows
```{r}
#| echo: TRUE

athlete_new_data <- data.frame(
  hurdles = 3,
  highjump = 1.5,
  shot = 9.5,
  run200m = 1.8,
  longjump = 6.8,
  javelin = 37,
  run800m = 25
)
predict(heptathlon_pca, athlete_new_data)[,1:2]
```

## PCA by Hand
To illustrate how PCA works, PCA is done on the `iris` data set without using dedicated PCA functions. The `iris` data set contains four continuous and one categorical variable, only the continuous variables are considered. First, the variables need to be mean-subtracted
```{r}
#| echo: TRUE

iris_centered <- apply(iris[,1:4], 2, function(x) x-mean(x))
```

Afterwards, the spectral decomposition of the covariance matrix is calculated using the `eigen()` function.
```{r}
#| echo: TRUE

iris_eigen <- iris_centered |>
  cov() |>
    eigen()
```

The eigenvalues correspond to the variance of the corresponding eigenvector. The eigenvector matrix is identical to the loadings calculated by `printcomp()`, except for the signs.
```{r}
iris_eigen$values |>
  round(2) |>
  matrix(nrow = 1) |>
    as.data.frame(row.names = "eigenvalues") |>
      kable(col.names = paste('PC', 1:4, sep = "")) |>
        kable_styling()
iris_eigen$vectors |>
  round(3) |>
    as.data.frame(row.names = colnames(iris_centered)) |>
      kable(col.names = paste('PC', 1:4, sep = ""),
            caption = "The eigenvectors correspond to the loadings of the principal components") |>
        kable_styling()
```

The scores are calculated from the data matrix and the eigenvector matrix
```{r}
#| echo: TRUE
#| eval: FALSE

iris_centered%*%iris_eigen$vectors
```
```{r}
iris_centered%*%iris_eigen$vectors |>
  round(2) |>
  kable(col.names = paste("PC", 1:4, sep=""), row.names = FALSE) |>
    kable_styling() |>
      scroll_box(height = '200px')
```

Dimension reduction is done by just taking the first two columns of the score matrix.
```{r}
#| echo: TRUE
#| eval: FALSE

iris_scores <- iris_centered%*%iris_eigen$vectors
colnames(iris_scores) <- paste("Comp.", 1:4, sep = "")
iris_scores[,1:2]
```

```{r}
iris_scores <- iris_centered%*%iris_eigen$vectors |>
  as.data.frame() |>
  cbind(factor(iris$Species))
colnames(iris_scores) <- c(paste("Comp.", 1:4, sep = ""), "species")

ggplot(iris_scores, aes(x = Comp.1, y = Comp.2, col = species)) +
  geom_point() +
  labs(
    title = "Dimension reduction of the iris data set"
  ) +
  gg_options
```


# Factor Analysis
Factor analysis serves to find underlying latent factors to observed variables. This is useful for cases in which the variable of interest cannot be measured directly (e.g. intelligence, social class) but is represented by some proxy variable (e.g. test scores, occupation and education).

## Factor Model {#sec-fa}
The factor model in matrix notation is as follows
$$
X =\pmb{\Lambda} f + u
$$
in which

* $X \in \mathbb{R}^q$ are the observed variables,
* $\pmb{\Lambda} \in \mathbb{R}^{q\times k}$ matrix containing the factor loadings,
* $f \in \mathbb{R}^k$ are the latent/common factors,
* $u \in \mathbb{R}^q$ are the specific factors.

$X$, $u$, and $f$ are random vectors, while $\Lambda$ is constant. It is important to note that $f$ is not observed, so $\Lambda$ and $u$ cannot be estimated with a least squares approach (in contrast to a regression model).

A number of things are assumed

* $E(u) = 0$
* $\Cov (u) = \Psi$, a diagonal matrix. The specific factors are uncorrelated.
* $\Cov (f_l, u_j) = 0$, the common and specific factors are uncorrelated.
* $E(X) = 0$, the data is mean-centered.
* $E(f) = 0$, $\Cov (f) = I$, latent factors are standardized with a mean of zero, and a variance of one. They are uncorrelated.

These assumptions imply that the common factors and the observed variables are uncorrelated. The correlation is established by the latent factors $\Lambda$.

\begin{align*}
\pmb{\Sigma} &= \Cov(\pmb{\Lambda} f+u) \\
&= \Cov(\pmb{\Lambda} f) + \Cov(u) \\
&= \pmb{\Lambda}\Cov(f)\pmb{\Lambda}^T+\pmb{\Psi}\\
&= \pmb{\Lambda\Lambda}^T + \pmb{\Psi}\\
\end{align*}

Writing as scalars, this equals
$$
\sigma^2_j = \Sigma_{l=1}^k \lambda^2_{jl} + \psi_j = h_j^2 + \psi_j
$$

in which $h_j^2$ is the communality, the variance resulting from the latent factors, and $\psi_j$ the specific variance, that is not shared among the observed variables. If the above expression holds, then the factor model holds for $X$. 

The factor model is scale-invariant, meaning that scaling the loadings and specific variances after applying factor analysis is equal to applying factor analysis to scaled variables.

\begin{align*}
\pmb{Y} &= \pmb{CX}, \pmb{C} = \mathrm{diag}(c_j)\\
\Cov(\pmb{Y}) &= \pmb{C}\Cov(\pmb{X})\pmb{C}^T \\
&= (\pmb{C\Lambda})(\pmb{C\Lambda})^T + \pmb{C\Psi C}^T \\
&= \tilde{\pmb{\Lambda}}\tilde{\pmb{\Lambda}}^T + \tilde{\pmb{\Psi}}\\
\tilde{\pmb{\Lambda}} &= \pmb{C\Lambda}, \tilde{\pmb{\Psi}} = \pmb{C\Psi C}^T
\end{align*}

## Estimation
Both the factor loadings $\pmb{\Lambda}$ and the specific variances $\pmb{\Psi}$ require estimation. They are connected to the covariance matrix $\pmb{\Sigma}$ by the expression
$$
\pmb{\Sigma} = \pmb{\Lambda\Lambda}^T + \pmb{\Psi}
$$

Estimation of both $\pmb{\Lambda}$ and $\pmb{\Psi}$ from this equation starts by initializing $\hat{\pmb{\Psi}}$ with
$$
\hat{\psi}_j = 1-\hat{h}_j^2
$$
in which $\hat{h}_j^2$ is either the square of the multiple correlation coefficient of the $j$-th variable with all other variables or the largest correlation coefficient between $j$ and another variable. Subsequently, $\pmb{\Lambda}$ can be estimated by spectral decomposition (using the correlation matrix $\pmb{R}$ instead of the covariance matrix) of
$$
\pmb{R}-\hat{\pmb{\Psi}} = \hat{\pmb{\Lambda}}\hat{\pmb{\Lambda}}^T = \pmb{A\Omega A}^T = \left(\pmb{A\Omega^\frac{1}{2}}\right) \left(\pmb{A\Omega^\frac{1}{2}}\right)^T
$$
Only the first $k$ eigenvalues and eigenvectors are kept, corresponding to the number of factors chosen for the model. These reduced eigenvalues and eigenvectors are used to populate the loadings \pmb{\Lambda}
$$
\hat{\pmb{\Lambda}}  = \pmb{A_1\Omega_1}^\frac{1}{2}
$$

This estimate is then used to estimate the specific factors $\hat{\pmb{\Psi}}$
$$
\hat{\pmb{\Psi}} = \pmb{R} - \hat{\pmb{\Lambda}}\hat{\pmb{\Lambda}}^T 
$$
$\hat{\pmb{\Lambda}}$ and $\hat{\pmb{\Psi}}$ are iteratively re-estimated until convergence. Alternatively, parameter estimation can be done by using maximum likelyhood estimation, assuming that the variables follow a multivariate normal distribution.

$$
l\left(\pmb{\Psi}, \pmb{\Lambda}\right) = -\frac{n}{2}\left(\log\left(2\pi\pmb{\Sigma}\right)+\Tr{\pmb{\Sigma}^{-1}S}\right)
$$
Factor analysis using the maximum likelyhood approach is available in `R` via the `factanal` function.

## Heywood Cases
It is possible that some $\hat{\psi}_j < 0$ or $\hat{\psi}_j > 1$. These cases are called _Heywood cases_ and violate that variances need to be positive and the normalization of the variance per variable
$$
 1 = \hat{h}_j^2 + \hat{\psi}_j
$$
Heywood cases can be approached by 

- collecting more data,
- using a different model (number of factors, variables),
- trying a different estimation technique.

## Number of Factors
Hypothesis testing can be used to test whether the chosen number of factors is sufficient. The hypotheses are
\begin{align*}
H_0&: \text{$k$-factor model holds true} \\
H_1&: \text{$\pmb{\Sigma}$ is more general (higher k necessary)}\\
\end{align*}
This hypothesis test included in the `R` output of factor analysis by default.

## Interpretation of Factor Loadings
There is no precise way of interpreting factor loadings, it can rather involve creativity and be debatable. Interpretation of factor loadings may be facilitated by rotations, which may change the loadings to be more interpretable. This is possible due to the _non-uniqueness_ of a factor model.

Assuming a rotation matrix $\pmb{M}$ and transformations $f^* = \pmb{M}^Tf$ and $\pmb{\Lambda}^* = \pmb{\Lambda}\pmb{M}$, it can be shown that the resulting factor model is identical to the untransformed model.

\begin{align*}
\pmb{X}^* &= \pmb{\Lambda}^* f^* + u = \left(\pmb{\Lambda}\pmb{M}\right)\left(\pmb{M}^T f\right) + u = \pmb{\Lambda} f + u = \pmb{X} \\
\pmb{\Sigma}^* &= \pmb{\Lambda}^*{\pmb{\Lambda}^*}^T + \pmb{\Psi} = \left(\pmb{\Lambda}\pmb{M}\right)\left(\pmb{\Lambda}\pmb{M}\right)^T + \pmb{\Psi} = \pmb{\Lambda}\pmb{\Lambda}^T + \pmb{\Psi} =\pmb{\Sigma}
\end{align*}

Non-uniqueness can be approached by imposing restrictions on the model parametrization, which is commonly done by estimation procedures. In addition, the rotations mentioned above can be used to increase the interpretability of the factor loadings. Interpretation is made easier if every variable has a high loading on at most one factor, and if all factor loadings are either large and positive, or zero. Two types of rotations exist that attempt to acheive this.

- **Orthogonal rotations** require the factors to be uncorrelated. This is useful, since the loadings still represent covariances between latent factors and observed variables. The _varimax_ rotation is a commonly used orthogonal rotation and default in the `factanal` function of `R`.
- **Oblique rotations** allow the factors to be correlated, as it may lead to better interpretable factors. The _promax_ rotation is an example for an oblique rotation which aims at a simple structure with low correlation between factors.

## Estimation of Factor Scores
It is possible to estimate factor scores for individual observations that represent scores of latent variables. Multiple methods of estimating factor scores exist, but they often yield very similar results in practice.

**Thompson's method** estimates factor scores $\hat{f}_i \in \mathbb{R}^k$ by
$$
\hat{f}_i = \hat{\pmb{\Lambda}}^T\hat{\pmb{\Sigma}}^{-1}x_i
$$
which is available via the option `scores="regression"` in the `R` function `factanal`.

## Comparison between FA & PCA
Comparing the models of factor analysis and principle component analysis
\begin{align*}
\text{FA: } \pmb{X} &= \pmb{\Lambda}f + u\\
\text{PCA: } X &= \pmb{A}_1^T Y^{(1)}+ \pmb{A}_2^T Y^{(2)} =  \pmb{A}_1^T Y^{(1)} + e
\end{align*}
shows that one of the differences is that $\Cov(u)$ is diagonal by assumption, while $\Cov(e)$ is usually not diagonal.

- FA and PCA reduce the dimensionality of data
- Do not yield results if the observed variables are uncorrelated
  - PC components are similar to the original variables
  - FA cannot find variance shared among the variables, thus the specific variance will be close to one
- The results of PCA and FA become more similar, the smaller the specific variance is. The results become identical (up to rotation) if the specific variance is zero
$$
\pmb{\Sigma} = \pmb{\Lambda}\pmb{\Lambda}^T + \pmb{\Psi} \overset{\pmb{\Psi} = 0}{=}  \pmb{\Lambda}\pmb{\Lambda}^T 
$$
The resulting spectral decomposition is the same for both PCA and FA


# Independent Component Analysis
Independent component analysis (ICA) allows to disentangle observed data that has been created by mixing different (latent) sources. This could be applied to a party, in which microphones placed in a room record a mixture of voices. ICA allows to separate individual voices from the mixed voice data. 

## Key Idea
The key idea of ICA to identify individual signals in a mixture is to use **non-Gaussianity** as measure of individual signals. This is motivated by the Central Limit Theorem, according to which mixtures of non-Gaussian distributions are more Gaussian than individual non-Gaussian distributions. By maximizing non-Gaussianity, individual sources can be separated from the mixtures.

Independent component analysis is a non-Gaussian alternative to PCA and FA. In addition, using the non-Gaussianity criterion allows to determine latent factors more uniquely than FA. 

## Independent Component Model
The independent component model is a linear, and noise-free model, that assumes that a mixing matrix $\pmb{B} \in \mathbb{R}^{q\times k}$ mixes the latent sources $S \in \mathbb{R}^k$ using linear combinations into the observed variables $X \in \mathbb{R}^q$, given that $E(\pmb{X}) = 0$.

$$
X = \pmb{B}S
$$

The goal is to find an **unmixing matrix** $\pmb{W} \in \mathbb{R}^{k\times q}$ that can separate the observed data into the latent sources. $X$ and $S$ are random vectors, while $\pmb{B}$ and $\pmb{W}$ are constant.

$$
S = \pmb{W}X
$$

Observed variables need to be **pre-whitened** to be used for ICA, as this reduces the number of parameters that have to be estimated. Pre-whitening transforms the observed data $X$ such that the transformed, pre-whitened variables $X^W$ have a variance of one and are uncorrelated to each other. Pre-whitening can be achieved by using the result of the spectral decomposition of the data covariance matrix $\Cov(X) = \pmb{\Sigma}$.
$$
X^W = \pmb{\Sigma}^{-\frac{1}{2}}X = \pmb{AD}^{-\frac{1}{2}}\pmb{A}^TX
$$
Due to the pre-whitening, the covariance matrix of the transformed data is $\Cov(X^W) = \pmb{I}$. The pre-whitened data does not correspond to the latent signals, as rotation of $X^W$ by any rotation matrix does not change the structure of the covariance matrix $\Cov(\pmb{OX}^W) = \pmb{OO}^T = \pmb{I}$. 

Key to the independent component analysis is that if the model holds true, there is a unique matrix $\pmb{M} \in \mathbb{R}^{q\times k}$ with $\pmb{M}^T\pmb{M} = I_k$ such that 
$$
\pmb{M}^TX^W = S
$$

Combining this with the pre-whitening, an unmixing matrix $\pmb{W}$ can be found
$$
S = \pmb{M}^TX^W = \pmb{M}^T\pmb{AD}^{-\frac{1}{2}}\pmb{A}^TX = \pmb{W}X
$$
Since $X^W$ can be inferred from the covariance matrix of $X$, only $\pmb{M}^T$ has to be estimated. This estimation is based on the non-Gaussianity assumption mentioned previously, and allows to uniquely identify the latent sources $S$. If the sources are gaussian distributed, the latent sources can only be identified up to rotations (as $\pmb{M}^T$ is not identifiable). 

## Estimation
The independent component model is estimated by rotating the pre-whitened data $X^W$ with the matrix $\pmb{M}^T$ such that the non-Gaussianity of the resulting latent sources $S$ are maximal.

The **excess kurtosis** $\kappa$ 
$$
\kappa(Y) = E\left(Y^4\right)-3
$$

is a measure of deviation from the Gaussian distribution, in which positive kurtosis indicates that the underlying distribution has larger tails than a Gaussian distribution and thus produces more outliers, while negative kurtosis indicates smaller tails and thus less outliers.

Finding a component can be accomplished by choosing $M_1 = \left(M_{11}, \dots, M_{1q}\right)^T$  such that the linear combination of $M_1$ and $X^W$ has maximal kurtosis, under the condition that $M_1$ is normalized.
$$
M_1 = \argmax_{M_1\in\mathbb{R}^q:|M_1|_2=1} \kappa\left(M_1^TX^W\right)
$$

Importantly, the relation
$$
\kappa\left(M_1^TX^W\right) \leq \max_{1\leq j\leq k}\kappa\left(S_j\right)
$$
justifies that a source is found when the kurtosis is maximized.

Estimating all the $M_j$ can be done either iteratively for the individual $M_j$ or for all the $M_j$ together. The iterative approach is  called _one-by-one_ or _deflation_ approach, and requires that $M_j$ are orthogonal to previous components. 
$$
M_j = \argmax_{M_j\in\mathbb{R}^q:|M_j|_2=1} \kappa\left(M_j^TX^W\right)
$$
for which
$$
M_j^TM_i = 0
$$
with $i = 1, \dots, j -2$. Algorithmic implementations are dependent on initial values, thus rerunning the algorithm multiple times with different initial values or seeds may be helpful.

The joint approach for estimating $M_j$ is called _symmetric_ or _parallel_ approach, and maximizes the sum of all kurtoses.
$$
\pmb{M} = \argmax_{|M_1|_2=1,\dots,|M_k|_2=1} \kappa\left(M_1^TX^W\right)+ \dots + \kappa\left(M_k^TX^W\right)
$$

## Example
Example images (cropped to same dimensions)
```{r}
library(imager)

crop_center <- function(df, dimvec) {
  ddf <- dim(df)
  m <- as.matrix(df)
  r1 <- ((ddf[1]-dimvec[1])/2):((ddf[1]+dimvec[1])/2)
  r2 <- ((ddf[2]-dimvec[2])/2):((ddf[2]+dimvec[2])/2)
  res <- m[r1,r2]
  res <- res[1:dimvec[1], 1:dimvec[2]]
  res
}

imMtoDF <- function(m){
  m <- as.matrix(m)
  cols <- rep(ncol(m):1, each = nrow(m))
  rows <- rep(1:nrow(m), ncol(m))
  df <- data.frame(x = rows, y = cols, z = as.vector(m))
  df
}

p1 <- load.image('data/Pablo_picasso.jpg')
p2 <- load.image('data/Michael_Jackson.jpg')

ica_p_dims <- c(min(dim(p1)[1], dim(p2)[1]), min(dim(p1)[2], dim(p2)[2]))

p1 <- imMtoDF(crop_center(p1, ica_p_dims))
p2 <- imMtoDF(crop_center(p2, ica_p_dims))

ica_g_options <- list(
  geom_tile(),
  scale_fill_gradient(low = 'black', high = 'white'),
  theme(legend.position = 'none')
)

ica_g1 <- ggplot(p1, aes(x = x, y = y, fill = z)) +
  ica_g_options

ica_g2 <- ggplot(p2, aes(x = x, y = y, fill = z)) +
  ica_g_options

plot_grid(ica_g1, ica_g2)


```

```{r}
ica_weights <- list(c(0.2, 0.8), c(0.7, 0.3), c(0.4, 0.1), c(0.1, 0.3))

set.seed(78)
tot_pix <- nrow(p1)
ica_mixed_images <- lapply(ica_weights, function(x) {
  p1_temp <- p1
  p2_temp <- p2
  new_df <- p1
  new_df$z <- x[1] * p1_temp$z + x[2] * p2_temp$z + rnorm(tot_pix, sd = 0.1)
  new_df
  })

ica_mixed_ps <- lapply(ica_mixed_images, function(x){
  ggplot(x, aes(x = x, y = y, fill = z)) +
  ica_g_options
})

plot_grid(plotlist = ica_mixed_ps)
```

```{r}
ica_dfmixed <- do.call(cbind, ica_mixed_images)

library(fastICA)
ica <- fastICA(ica_dfmixed, n.comp = 4, alg.typ = 'deflation', method = "C", fun = "logcosh")
```

```{r}
ica_res <- lapply(1:ncol(ica$S), function(x){
  imMtoDF(matrix(ica$S[,x], nrow = dim(p1m)[1], ncol = dim(p1m)[2]))
})

ica_res_m <- do.call(rbind, ica_res)
ica_res_m$plot <- rep(1:4, each = nrow(ica_res[[1]]))

## probably should normalize grey values to be between 0 and 1
ggplot(ica_res_m, aes(x = x, y = y, fill = z)) +
  geom_tile() +
  facet_wrap(~plot) +
  scale_fill_gradient(low = 'black', high = 'white') 
  # theme(legend.position = 'none')

ica_mixed_res <- lapply(ica_res, function(x){
  ggplot(x, aes(x = x, y = y, fill = z)) +
  geom_tile() +
  scale_fill_gradient(low = 'black', high = 'white') +
  theme(legend.position = 'none')
})

plot_grid(plotlist = ica_mixed_res)
```

